
@INPROCEEDINGS{Auer2007-ie,
  title     = "{DBpedia}: A Nucleus for a Web of Open Data",
  booktitle = "The Semantic Web",
  author    = "Auer, S{\"o}ren and Bizer, Christian and Kobilarov, Georgi and
               Lehmann, Jens and Cyganiak, Richard and Ives, Zachary",
  abstract  = "DBpedia is a community effort to extract structured information
               from Wikipedia and to make this information available on the
               Web. DBpedia allows you to ask sophisticated queries against
               datasets derived from Wikipedia and to link other datasets on
               the Web to Wikipedia data. We describe the extraction of the
               DBpedia datasets, and how the resulting information is published
               on the Web for human- and machine-consumption. We describe some
               emerging applications from the DBpedia community and show how
               website authors can facilitate DBpedia content within their
               sites. Finally, we present the current status of interlinking
               DBpedia with other open datasets on the Web and outline how
               DBpedia could serve as a nucleus for an emerging Web of open
               data.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "722--735",
  year      =  2007
}

@ARTICLE{Doshi-Velez2017-ec,
  title         = "Towards A Rigorous Science of Interpretable Machine Learning",
  author        = "Doshi-Velez, Finale and Kim, Been",
  abstract      = "As machine learning systems become ubiquitous, there has
                   been a surge of interest in interpretable machine learning:
                   systems that provide explanation for their outputs. These
                   explanations are often used to qualitatively assess other
                   criteria such as safety or non-discrimination. However,
                   despite the interest in interpretability, there is very
                   little consensus on what interpretable machine learning is
                   and how it should be measured. In this position paper, we
                   first define interpretability and describe when
                   interpretability is needed (and when it is not). Next, we
                   suggest a taxonomy for rigorous evaluation and expose open
                   questions towards a more rigorous science of interpretable
                   machine learning.",
  month         =  feb,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1702.08608"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Rydning2018-mt,
  title     = "The digitization of the world from edge to core",
  author    = "Rydning, David Reinsel--John Gantz--John",
  abstract  = "This process of digitization is often referred to as digital
               transformation, and it is profoundly changing the shape of
               business today, impacting companies in every industry and
               consumers around the world . Digital transformation is not about
               the evolution of devices …",
  journal   = "Framingham: International Data Corporation",
  publisher = "storelabs.org",
  year      =  2018
}

@ARTICLE{Codd1970-vj,
  title     = "A relational model of data for large shared data banks",
  author    = "Codd, E F",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  13,
  number    =  6,
  pages     = "377--387",
  month     =  jun,
  year      =  1970,
  address   = "New York, NY, USA",
  keywords  = "hierarchies of data, redundancy, predicate calculus,
               consistency, data integrity, data structure, join, data bank,
               derivability, security, composition, data organization,
               relations, networks of data, retrieval language, data base"
}

@INCOLLECTION{Lundberg2017-wo,
  title     = "A Unified Approach to Interpreting Model Predictions",
  booktitle = "Advances in Neural Information Processing Systems 30",
  author    = "Lundberg, Scott M and Lee, Su-In",
  editor    = "Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and
               Fergus, R and Vishwanathan, S and Garnett, R",
  publisher = "Curran Associates, Inc.",
  pages     = "4765--4774",
  year      =  2017
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Shapley1953-cc,
  title     = "A value for n-person games",
  author    = "Shapley, Lloyd S",
  abstract  = "At the foundation of the theory of games is the assumption that
               the players of a game can evaluate, in their utility scales,
               every`` prospect'' that might arise as a result of a play. In
               attempting to apply the theory to any field, one would normally
               expect to be permitted to …",
  journal   = "Contributions to the Theory of Games",
  publisher = "books.google.com",
  volume    =  2,
  number    =  28,
  pages     = "307--317",
  year      =  1953
}

@ARTICLE{Medin1978-jk,
  title    = "Context theory of classification learning",
  author   = "Medin, Douglas L and Schaffer, Marguerite M",
  abstract = "Most theories dealing with ill-defined concepts assume that
              performance is based on category level information or a mixture
              of category level and specific item information. A context theory
              of classification is described in which judgments are assumed to
              derive exclusively from stored exemplar information. The main
              idea is that a probe item acts as a retrieval cue to access
              information associated with stimuli similar to the probe. The
              predictions of the context theory are contrasted with those of a
              class of theories (including prototype theory) that assume that
              the information entering into judgments can be derived from an
              additive combination of information from component cue
              dimensions. Across 4 experiments with 128 paid Ss, using both
              geometric forms and schematic faces as stimuli, the context
              theory consistently gave a better account of the data. The
              relation of context theory to other theories and phenomena
              associated with ill-defined concepts is discussed in detail. (42
              ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)",
  journal  = "Psychol. Rev.",
  volume   =  85,
  number   =  3,
  pages    = "207--238",
  month    =  may,
  year     =  1978
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@TECHREPORT{Porter1990-is,
  title       = "Concept learning and heuristic classification in weak-theory
                 domains",
  author      = "Porter, Bruce W and Bareiss, Ray and Holte, Robert C",
  publisher   = "TEXAS UNIV AT AUSTIN …",
  institution = "TEXAS UNIV AT AUSTIN ARTIFICIAL INTELLIGENCE LAB",
  year        =  1990
}

@BOOK{Molnar2020-do,
  title     = "Interpretable Machine Learning",
  author    = "Molnar, Christoph",
  abstract  = "This book is about making machine learning models and their
               decisions interpretable. After exploring the concepts of
               interpretability, you will learn about simple, interpretable
               models such as decision trees, decision rules and linear
               regression. Later chapters focus on general model-agnostic
               methods for interpreting black box models like feature
               importance and accumulated local effects and explaining
               individual predictions with Shapley values and LIME. All
               interpretation methods are explained in depth and discussed
               critically. How do they work under the hood? What are their
               strengths and weaknesses? How can their outputs be interpreted?
               This book will enable you to select and correctly apply the
               interpretation method that is most suitable for your machine
               learning project.",
  publisher = "Lulu.com",
  month     =  feb,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Pearson1901-de,
  title     = "{LIII}. On lines and planes of closest fit to systems of points
               in space",
  author    = "Pearson, Karl",
  journal   = "The London, Edinburgh, and Dublin Philosophical Magazine and
               Journal of Science",
  publisher = "Taylor \& Francis",
  volume    =  2,
  number    =  11,
  pages     = "559--572",
  month     =  nov,
  year      =  1901
}

@BOOK{Mitkov2004-fz,
  title     = "The Oxford Handbook of Computational Linguistics",
  author    = "Mitkov, Ruslan",
  abstract  = "Thirty-eight chapters, comissioned from experts all over the
               world, describe major concepts, methods, and applications in
               computational linguistics. Part I, Linguistic Fundamentals,
               provides an overview of the field suitable for senior
               undergraduates and non-specialists from other fields of
               linguistics and related disciplines. Part II describes current
               tasks, techniques, and tools in Natural Language Processing and
               aims to meet the needs of post-doctoral workers and others
               embarking on computational language research. Part III surveys
               current applications. This book is a state-of-the-art reference
               to one of the most active and productive fields in linguistics.
               It will be of interest and practical use to a wide range of
               linguists, as well as to researchers in such fields as
               informatics, artificial intelligence, language engineering, and
               cognitive science.",
  publisher = "OUP Oxford",
  year      =  2004,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Sheng2008-gk,
  title     = "Get another label? improving data quality and data mining using
               multiple, noisy labelers",
  booktitle = "Proceedings of the 14th {ACM} {SIGKDD} international conference
               on Knowledge discovery and data mining",
  author    = "Sheng, Victor S and Provost, Foster and Ipeirotis, Panagiotis G",
  abstract  = "This paper addresses the repeated acquisition of labels for data
               items when the labeling is imperfect. We examine the improvement
               (or lack thereof) in data quality via repeated labeling, and
               focus especially on the improvement of training labels for
               supervised …",
  publisher = "Association for Computing Machinery",
  pages     = "614--622",
  series    = "KDD '08",
  month     =  aug,
  year      =  2008,
  address   = "New York, NY, USA",
  keywords  = "data selection, data preprocessing",
  location  = "Las Vegas, Nevada, USA"
}

@ARTICLE{Trunk1979-sq,
  title     = "A problem of dimensionality: a simple example",
  author    = "Trunk, G V",
  abstract  = "In pattern recognition problems it has been noted that beyond a
               certain point the inclusion of additional parameters (that have
               been estimated) leads to higher probabilities of error. A simple
               problem has been formulated where the probability of error
               approaches zero as the dimensionality increases and all the
               parameters are known; on the other hand, the probability of
               error approaches one-half as the dimensionality increases and
               parameters are estimated.",
  journal   = "IEEE Trans. Pattern Anal. Mach. Intell.",
  publisher = "ieeexplore.ieee.org",
  volume    =  1,
  number    =  3,
  pages     = "306--307",
  month     =  mar,
  year      =  1979,
  language  = "en"
}

@ARTICLE{Batini2009-aa,
  title     = "Methodologies for data quality assessment and improvement",
  author    = "Batini, Carlo and Cappiello, Cinzia and Francalanci, Chiara and
               Maurino, Andrea",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  41,
  number    =  3,
  pages     = "1--52",
  month     =  jul,
  year      =  2009,
  address   = "New York, NY, USA",
  keywords  = "data quality assessment, quality dimension, information system,
               methodology, Data quality, data quality improvement, data
               quality measurement"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Rahm2000-fz,
  title     = "Data cleaning: Problems and current approaches",
  author    = "Rahm, Erhard and Do, Hong Hai",
  abstract  = "We classify data quality problems that are addressed by data
               cleaning and provide an overview of the main solution
               approaches. Data cleaning is especially required when
               integrating heterogeneous data sources and should be addressed
               together with schema …",
  publisher = "academia.edu",
  year      =  2000
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Lundberg2017-ti,
  title     = "A Unified Approach to Interpreting Model Predictions",
  booktitle = "Advances in Neural Information Processing Systems 30",
  author    = "Lundberg, Scott M and Lee, Su-In",
  editor    = "Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and
               Fergus, R and Vishwanathan, S and Garnett, R",
  abstract  = "Understanding why a model makes a certain prediction can be as
               crucial as the prediction's accuracy in many applications.
               However, the highest accuracy for large modern datasets is often
               achieved by complex models that even experts struggle to
               interpret, such as ensemble or deep learning models, creating a
               tension between accuracy and interpretability. In response,
               various methods have recently been proposed to help users
               interpret the predictions of complex models, but it is often
               unclear how these methods are related and …",
  publisher = "Curran Associates, Inc.",
  pages     = "4765--4774",
  year      =  2017
}

@ARTICLE{Pedregosa2011-su,
  title   = "Scikit-learn: Machine Learning in {P}ython",
  author  = "Pedregosa, F and Varoquaux, G and Gramfort, A and Michel, V and
             Thirion, B and Grisel, O and Blondel, M and Prettenhofer, P and
             Weiss, R and Dubourg, V and Vanderplas, J and Passos, A and
             Cournapeau, D and Brucher, M and Perrot, M and Duchesnay, E",
  journal = "J. Mach. Learn. Res.",
  volume  =  12,
  pages   = "2825--2830",
  year    =  2011
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@UNPUBLISHED{Lipton2018-nr,
  title    = "The Mythos of Model Interpretability",
  author   = "Lipton, Zachary C",
  abstract = "… interpretations often do not elucidate precisely how a model
              works, they may nonetheless confer useful information for
              practitioners and end users of machine learning … To the extent
              that we might consider humans to be interpretable , this is the
              sort of interpretability that applies …",
  month    =  jun,
  year     =  2018
}

@INPROCEEDINGS{Strumbelj2011-fz,
  title     = "A General Method for Visualizing and Explaining {Black-Box}
               Regression Models",
  booktitle = "Adaptive and Natural Computing Algorithms",
  author    = "{\v S}trumbelj, Erik and Kononenko, Igor",
  abstract  = "We propose a method for explaining regression models and their
               predictions for individual instances. The method successfully
               reveals how individual features influence the model and can be
               used with any type of regression model in a uniform way. We used
               different types of models and data sets to demonstrate that the
               method is a useful tool for explaining, comparing, and
               identifying errors in regression models.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "21--30",
  year      =  2011
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@UNPUBLISHED{Guidotti2018-ph,
  title    = "A Survey of Methods for Explaining Black Box Models",
  author   = "Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and
              Turini, Franco and Giannotti, Fosca and Pedreschi, Dino",
  abstract = "In recent years, many accurate decision support systems have been
              constructed as black boxes, that is as systems that hide their
              internal logic to the user. This lack of explanation constitutes
              both a practical and an ethical issue. The literature reports
              many approaches …",
  number   = "Article 93",
  month    =  aug,
  year     =  2018,
  keywords = "Open the black box, transparent models, interpretability,
              explanations"
}

@INPROCEEDINGS{Nasreen2014-sz,
  title     = "A Survey Of Feature Selection And Feature Extraction Techniques
               In Machine {Learning,SAI,2014}",
  booktitle = "Science and Information ({SAI})",
  author    = "Nasreen, Shamila",
  abstract  = "PDF | Dimensionality reduction as a preprocessing step to
               machine learning is effective in removing irrelevant and
               redundant data, increasing learning... | Find, read and cite all
               the research you need on ResearchGate",
  publisher = "unknown",
  month     =  aug,
  year      =  2014
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@UNPUBLISHED{Li2017-vo,
  title    = "Feature Selection: A Data Perspective",
  author   = "Li, Jundong and Cheng, Kewei and Wang, Suhang and Morstatter,
              Fred and Trevino, Robert P and Tang, Jiliang and Liu, Huan",
  abstract = "… selection algorithms to cover all components mentioned in
              Figure 2. We also release a feature selection repository in
              Python , named scikit … It includes near 40 representative
              feature selection algorithms. The web page of the repository is
              available at http:// featureselection .asu.edu …",
  number   = "Article 94",
  month    =  dec,
  year     =  2017,
  keywords = "Feature selection"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Hall2000-bx,
  title     = "Correlation-based feature selection of discrete and numeric
               class machine learning",
  author    = "Hall, Mark A",
  abstract  = "Algorithms for feature selection fall into two broad categories:
               wrappers that use the learning algorithm itself to evaluate the
               usefulness of features and filters that evaluate features
               according to heuristics based on general characteristics of the
               data. For application to large …",
  publisher = "University of Waikato, Department of Computer Science",
  year      =  2000
}

@INPROCEEDINGS{Zien2009-wm,
  title     = "The Feature Importance Ranking Measure",
  booktitle = "Machine Learning and Knowledge Discovery in Databases",
  author    = "Zien, Alexander and Kr{\"a}mer, Nicole and Sonnenburg, S{\"o}ren
               and R{\"a}tsch, Gunnar",
  abstract  = "Most accurate predictions are typically obtained by learning
               machines with complex feature spaces (as e.g. induced by
               kernels). Unfortunately, such decision rules are hardly
               accessible to humans and cannot easily be used to gain insights
               about the application domain. Therefore, one often resorts to
               linear models in combination with variable selection, thereby
               sacrificing some predictive power for presumptive
               interpretability. Here, we introduce the Feature Importance
               Ranking Measure (FIRM), which by retrospective analysis of
               arbitrary learning machines allows to achieve both excellent
               predictive performance and superior interpretation. In contrast
               to standard raw feature weighting, FIRM takes the underlying
               correlation structure of the features into account. Thereby, it
               is able to discover the most relevant features, even if their
               appearance in the training data is entirely prevented by noise.
               The desirable properties of FIRM are investigated analytically
               and illustrated in simulations.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "694--709",
  year      =  2009
}

@ARTICLE{Altmann2010-lq,
  title     = "Permutation importance: a corrected feature importance measure",
  author    = "Altmann, Andr{\'e} and Tolo{\c s}i, Laura and Sander, Oliver and
               Lengauer, Thomas",
  abstract  = "MOTIVATION: In life sciences, interpretability of machine
               learning models is as important as their prediction accuracy.
               Linear models are probably the most frequently used methods for
               assessing feature relevance, despite their relative
               inflexibility. However, in the past years effective estimators
               of feature relevance have been derived for highly complex or
               non-parametric models such as support vector machines and
               RandomForest (RF) models. Recently, it has been observed that RF
               models are biased in such a way that categorical variables with
               a large number of categories are preferred. RESULTS: In this
               work, we introduce a heuristic for normalizing feature
               importance measures that can correct the feature importance
               bias. The method is based on repeated permutations of the
               outcome vector for estimating the distribution of measured
               importance for each variable in a non-informative setting. The
               P-value of the observed importance provides a corrected measure
               of feature importance. We apply our method to simulated data and
               demonstrate that (i) non-informative predictors do not receive
               significant P-values, (ii) informative variables can
               successfully be recovered among non-informative variables and
               (iii) P-values computed with permutation importance (PIMP) are
               very helpful for deciding the significance of variables, and
               therefore improve model interpretability. Furthermore, PIMP was
               used to correct RF-based importance measures for two real-world
               case studies. We propose an improved RF model that uses the
               significant variables with respect to the PIMP measure and show
               that its prediction accuracy is superior to that of other
               existing models. AVAILABILITY: R code for the method presented
               in this article is available at http://www.mpi-inf.mpg.de/
               approximately altmann/download/PIMP.R CONTACT:
               altmann@mpi-inf.mpg.de, laura.tolosi@mpi-inf.mpg.de
               SUPPLEMENTARY INFORMATION: Supplementary data are available at
               Bioinformatics online.",
  journal   = "Bioinformatics",
  publisher = "academic.oup.com",
  volume    =  26,
  number    =  10,
  pages     = "1340--1347",
  month     =  may,
  year      =  2010,
  language  = "en"
}

@INPROCEEDINGS{Chu2015-fs,
  title     = "{KATARA}: A Data Cleaning System Powered by Knowledge Bases and
               Crowdsourcing",
  booktitle = "Proceedings of the 2015 {ACM} {SIGMOD} International Conference
               on Management of Data",
  author    = "Chu, Xu and Morcos, John and Ilyas, Ihab F and Ouzzani, Mourad
               and Papotti, Paolo and Tang, Nan and Ye, Yin",
  publisher = "Association for Computing Machinery",
  pages     = "1247--1261",
  series    = "SIGMOD '15",
  month     =  may,
  year      =  2015,
  address   = "New York, NY, USA",
  keywords  = "data cleaning, crowdsourcing, knowledge base, data quality",
  location  = "Melbourne, Victoria, Australia"
}

@ARTICLE{Abedjan2015-ul,
  title    = "Profiling relational data: a survey",
  author   = "Abedjan, Ziawasch and Golab, Lukasz and Naumann, Felix",
  abstract = "Profiling data to determine metadata about a given dataset is an
              important and frequent activity of any IT professional and
              researcher and is necessary for various use-cases. It encompasses
              a vast array of methods to examine datasets and produce metadata.
              Among the simpler results are statistics, such as the number of
              null values and distinct values in a column, its data type, or
              the most frequent patterns of its data values. Metadata that are
              more difficult to compute involve multiple columns, namely
              correlations, unique column combinations, functional
              dependencies, and inclusion dependencies. Further techniques
              detect conditional properties of the dataset at hand. This survey
              provides a classification of data profiling tasks and
              comprehensively reviews the state of the art for each class. In
              addition, we review data profiling tools and systems from
              research and industry. We conclude with an outlook on the future
              of data profiling beyond traditional profiling tasks and beyond
              relational databases.",
  journal  = "VLDB J.",
  volume   =  24,
  number   =  4,
  pages    = "557--581",
  month    =  aug,
  year     =  2015
}

@ARTICLE{Sahay2019-mr,
  title         = "Schema Matching using Machine Learning",
  author        = "Sahay, Tanvi and Mehta, Ankita and Jadon, Shruti",
  abstract      = "Schema Matching is a method of finding attributes that are
                   either similar to each other linguistically or represent the
                   same information. In this project, we take a hybrid approach
                   at solving this problem by making use of both the provided
                   data and the schema name to perform one to one schema
                   matching and introduce the creation of a global dictionary
                   to achieve one to many schema matching. We experiment with
                   two methods of one to one matching and compare both based on
                   their F-scores, precision, and recall. We also compare our
                   method with the ones previously suggested and highlight
                   differences between them.",
  month         =  nov,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DB",
  eprint        = "1911.11543"
}

@INCOLLECTION{Berlin2013-wb,
  title     = "Database Schema Matching Using Machine Learning with Feature
               Selection",
  booktitle = "Seminal Contributions to Information Systems Engineering: 25
               Years of {CAiSE}",
  author    = "Berlin, Jacob and Motro, Amihai",
  editor    = "Bubenko, Janis and Krogstie, John and Pastor, Oscar and Pernici,
               Barbara and Rolland, Colette and S{\o}lvberg, Arne",
  abstract  = "Schema matching, the problem of finding mappings between the
               attributes of two semantically related database schemas, is an
               important aspect of many database applications such as schema
               integration, data warehousing, and electronic commerce.
               Unfortunately, schema matching remains largely a manual,
               labor-intensive process. Furthermore, the effort required is
               typically linear in the number of schemas to be matched; the
               next pair of schemas to match is not any easier than the
               previous pair. In this paper we describe a system, called
               Automatch, that uses machine learning techniques to automate
               schema matching. Based primarily on Bayesian learning, the
               system acquires probabilistic knowledge from examples that have
               been provided by domain experts. This knowledge is stored in a
               knowledge base called the attribute dictionary. When presented
               with a pair of new schemas that need to be matched (and their
               corresponding database instances), Automatch uses the attribute
               dictionary to find an optimal matching. We also report initial
               results from the Automatch project.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "315--329",
  year      =  2013,
  address   = "Berlin, Heidelberg"
}

@ARTICLE{KruseSebastian2018-kx,
  title     = "Efficient discovery of approximate dependencies",
  author    = "{KruseSebastian} and {NaumannFelix}",
  abstract  = "Functional dependencies (FDs) and unique column combinations
               (UCCs) form a valuable ingredient for many data management
               tasks, such as data cleaning, schema recovery, and query
               optimization. Becaus...",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment PUB4722",
  month     =  mar,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Koumarelas2020-oz,
  title     = "{MDedup}: duplicate detection with matching dependencies",
  author    = "Koumarelas, Loannis and Papenbrock, Thorsten and Naumann, Felix",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  13,
  number    =  5,
  pages     = "712--725",
  month     =  jan,
  year      =  2020
}

@MISC{Zhuoran_Yu_Georgia_Institute_of_Technology_Atlanta_GA_USA_undated-wi,
  title        = "{PIClean} | Proceedings of the 2019 International Conference
                  on Management of Data",
  author       = "{Zhuoran Yu Georgia Institute of Technology, Atlanta, GA,
                  USA} and {Xu Chu Georgia Institute of Technology, Atlanta,
                  GA, USA}",
  howpublished = "\url{https://dl.acm.org/doi/10.1145/3299869.3320214}",
  note         = "Accessed: 2020-2-13",
  language     = "en"
}

@BOOK{Osborne2013-kj,
  title    = "Best practices in data cleaning: A Complete Guide to Everything
              You Need to Do Before and After Collecting Your Data",
  author   = "Osborne, Jason W",
  abstract = "PDF | Many researchers jump straight from data collection to data
              analysis without realizing how analyses and hypothesis tests can
              go profoundly wrong... | Find, read and cite all the research you
              need on ResearchGate",
  month    =  jan,
  year     =  2013
}

@ARTICLE{Geerts2019-ml,
  title     = "Cleaning data with Llunatic",
  author    = "Geerts, Floris and Mecca, Giansalvatore and Papotti, Paolo and
               Santoro, Donatello",
  abstract  = "Data cleaning (or data repairing) is considered a crucial
               problem in many database-related tasks. It consists in making a
               database consistent with respect to a given set of constraints.
               In recent years, repairing methods have been proposed for
               several classes of constraints. These methods, however, tend to
               hard-code the strategy to repair conflicting values and are
               specialized toward specific classes of constraints. In this
               paper, we develop a general chase-based repairing framework,
               referred to as Llunatic, in which repairs can be obtained for a
               large class of constraints and by using different strategies to
               select preferred values. The framework is based on an elegant
               formalization in terms of labeled instances and partially
               ordered preference labels. In this context, we revisit concepts
               such as upgrades, repairs and the chase. In Llunatic, various
               repairing strategies can be slotted in, without the need for
               changing the underlying implementation. Furthermore, Llunatic is
               the first data repairing system which is DBMS-based. We report
               experimental results that confirm its good scalability and show
               that various instantiations of the framework result in repairs
               of good quality.",
  journal   = "VLDB J.",
  publisher = "Springer",
  month     =  nov,
  year      =  2019
}

@ARTICLE{Qi2018-tq,
  title         = "Impacts of Dirty Data: and Experimental Evaluation",
  author        = "Qi, Zhixin and Wang, Hongzhi and Li, Jianzhong and Gao, Hong",
  abstract      = "Data quality issues have attracted widespread attention due
                   to the negative impacts of dirty data on data mining and
                   machine learning results. The relationship between data
                   quality and the accuracy of results could be applied on the
                   selection of the appropriate algorithm with the
                   consideration of data quality and the determination of the
                   data share to clean. However, rare research has focused on
                   exploring such relationship. Motivated by this, this paper
                   conducts an experimental comparison for the effects of
                   missing, inconsistent and conflicting data on
                   classification, clustering, and regression algorithms. Based
                   on the experimental findings, we provide guidelines for
                   algorithm selection and data cleaning.",
  month         =  mar,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DB",
  eprint        = "1803.06071"
}

@ARTICLE{Asghar2015-yq,
  title   = "Automatic discovery of functional dependencies and conditional
             functional dependencies: A comparative study",
  author  = "Asghar, Nabiha and Ghenai, Amira",
  journal = "university of Waterloo",
  year    =  2015
}

@ARTICLE{Bartoli2016-hx,
  title     = "Inference of Regular Expressions for Text Extraction from
               Examples",
  author    = "Bartoli, A and De Lorenzo, A and Medvet, E and Tarlao, F",
  abstract  = "A large class of entity extraction tasks from text that is
               either semistructured or fully unstructured may be addressed by
               regular expressions, because in many practical cases the
               relevant entities follow an underlying syntactical pattern and
               this pattern may be described by a regular expression. In this
               work, we consider the long-standing problem of synthesizing such
               expressions automatically, based solely on examples of the
               desired behavior. We present the design and implementation of a
               system capable of addressing extraction tasks of realistic
               complexity. Our system is based on an evolutionary procedure
               carefully tailored to the specific needs of regular expression
               generation by examples. The procedure executes a search driven
               by a multiobjective optimization strategy aimed at
               simultaneously improving multiple performance indexes of
               candidate solutions while at the same time ensuring an adequate
               exploration of the huge solution space. We assess our proposal
               experimentally in great depth, on a number of challenging
               datasets. The accuracy of the obtained solutions seems to be
               adequate for practical usage and improves over earlier proposals
               significantly. Most importantly, our results are highly
               competitive even with respect to human operators. A prototype is
               available as a web application at http://regex.inginf.units.it.",
  journal   = "IEEE Trans. Knowl. Data Eng.",
  publisher = "ieeexplore.ieee.org",
  volume    =  28,
  number    =  5,
  pages     = "1217--1230",
  month     =  may,
  year      =  2016,
  keywords  = "genetic algorithms;text detection;regular expression
               inference;text extraction;realistic complexity extraction
               tasks;multiobjective optimization strategy;Proposals;Data
               mining;Training data;Training;Complexity
               theory;Optimization;Electronic mail;Genetic
               Programming;Information extraction;Programming by
               examples;Multiobjective optimization;Heuristic search;Genetic
               programming;information extraction;programming by
               examples;multiobjective optimization;heuristic search"
}

@INPROCEEDINGS{Chu2013-ob,
  title     = "Holistic data cleaning: Putting violations into context",
  booktitle = "2013 {IEEE} 29th International Conference on Data Engineering
               ({ICDE})",
  author    = "Chu, X and Ilyas, I F and Papotti, P",
  abstract  = "Data cleaning is an important problem and data quality rules are
               the most promising way to face it with a declarative approach.
               Previous work has focused on specific formalisms, such as
               functional dependencies (FDs), conditional functional
               dependencies (CFDs), and matching dependencies (MDs), and those
               have always been studied in isolation. Moreover, such techniques
               are usually applied in a pipeline or interleaved. In this work
               we tackle the problem in a novel, unified framework. First, we
               let users specify quality rules using denial constraints with
               ad-hoc predicates. This language subsumes existing formalisms
               and can express rules involving numerical values, with
               predicates such as ``greater than'' and ``less than''. More
               importantly, we exploit the interaction of the heterogeneous
               constraints by encoding them in a conflict hypergraph. Such
               holistic view of the conflicts is the starting point for a novel
               definition of repair context which allows us to compute
               automatically repairs of better quality w.r.t. previous
               approaches in the literature. Experimental results on real
               datasets show that the holistic approach outperforms previous
               algorithms in terms of quality and efficiency of the repair.",
  publisher = "ieeexplore.ieee.org",
  pages     = "458--469",
  month     =  apr,
  year      =  2013,
  keywords  = "constraint handling;data handling;graph theory;pattern
               matching;holistic data cleaning;data quality rule;declarative
               approach;conditional functional dependencies;CFD;matching
               dependencies;MD;denial constraint;ad-hoc predicate;numerical
               value;heterogeneous constraint;conflict hypergraph;data
               repair;Maintenance engineering;Databases;Cleaning;Context;Cities
               and towns;Remuneration;Proposals"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Dallachiesa2013-he,
  title       = "{NADEEF}: a commodity data cleaning system",
  booktitle   = "Proceedings of the 2013 {ACM} {SIGMOD} International
                 Conference on Management of Data",
  author      = "Dallachiesa, Michele and Ebaid, Amr and Eldawy, Ahmed and
                 Elmagarmid, Ahmed and Ilyas, Ihab F and Ouzzani, Mourad and
                 Tang, Nan",
  abstract    = "Despite the increasing importance of data quality and the rich
                 theoretical and practical contributions in all aspects of data
                 cleaning, there is no single end-to-end off-the-shelf solution
                 to (semi-) automate the detection and the repairing of
                 violations wrt a set of …",
  publisher   = "dl.acm.org",
  pages       = "541--552",
  institution = "ACM",
  year        =  2013
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Yakout2013-ww,
  title       = "Don't be {SCAREd}: use {SCalable} Automatic {REpairing} with
                 maximal likelihood and bounded changes",
  booktitle   = "Proceedings of the 2013 {ACM} {SIGMOD} International
                 Conference on Management of Data",
  author      = "Yakout, Mohamed and Berti-{\'E}quille, Laure and Elmagarmid,
                 Ahmed K",
  abstract    = "Various computational procedures or constraint-based methods
                 for data repairing have been proposed over the last decades to
                 identify errors and, when possible, correct them. However,
                 these approaches have several limitations including the
                 scalability and quality of …",
  publisher   = "dl.acm.org",
  pages       = "553--564",
  institution = "ACM",
  year        =  2013
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Geerts2013-ft,
  title     = "The {LLUNATIC} data-cleaning framework",
  author    = "Geerts, Floris and Mecca, Giansalvatore and Papotti, Paolo and
               Santoro, Donatello",
  abstract  = "Data-cleaning (or data-repairing) is considered a crucial
               problem in many database-related tasks. It consists in making a
               database consistent with respect to a set of given constraints.
               In recent years, repairing methods have been proposed for
               several classes of constraints. However, these methods rely on
               ad hoc decisions and tend to hard-code the strategy to repair
               conflicting values. As a consequence, there is currently no
               general algorithm to solve database repairing problems that
               involve different kinds of constraints and different …",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  6,
  number    =  9,
  pages     = "625--636",
  year      =  2013
}

@ARTICLE{Krishnan2017-mk,
  title         = "{BoostClean}: Automated Error Detection and Repair for
                   Machine Learning",
  author        = "Krishnan, Sanjay and Franklin, Michael J and Goldberg, Ken
                   and Wu, Eugene",
  abstract      = "Predictive models based on machine learning can be highly
                   sensitive to data error. Training data are often combined
                   with a variety of different sources, each susceptible to
                   different types of inconsistencies, and new data streams
                   during prediction time, the model may encounter previously
                   unseen inconsistencies. An important class of such
                   inconsistencies is domain value violations that occur when
                   an attribute value is outside of an allowed domain. We
                   explore automatically detecting and repairing such
                   violations by leveraging the often available clean test
                   labels to determine whether a given detection and repair
                   combination will improve model accuracy. We present
                   BoostClean which automatically selects an ensemble of error
                   detection and repair combinations using statistical
                   boosting. BoostClean selects this ensemble from an
                   extensible library that is pre-populated general detection
                   functions, including a novel detector based on the Word2Vec
                   deep learning model, which detects errors across a diverse
                   set of domains. Our evaluation on a collection of 12
                   datasets from Kaggle, the UCI repository, real-world data
                   analyses, and production datasets that show that Boost-
                   Clean can increase absolute prediction accuracy by up to 9\%
                   over the best non-ensembled alternatives. Our optimizations
                   including parallelism, materialization, and indexing
                   techniques show a 22.2x end-to-end speedup on a 16-core
                   machine.",
  month         =  nov,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DB",
  eprint        = "1711.01299"
}

@INPROCEEDINGS{Rammelaere2019-ps,
  title     = "Revisiting Conditional Functional Dependency Discovery:
               Splitting the ``C'' from the {``FD''}",
  booktitle = "Machine Learning and Knowledge Discovery in Databases",
  author    = "Rammelaere, Joeri and Geerts, Floris",
  abstract  = "Many techniques for cleaning dirty data are based on enforcing
               some set of integrity constraints. Conditional functional
               dependencies (CFDs) are a combination of traditional Functional
               dependencies (FDs) and association rules, and are widely used as
               a constraint formalism for data cleaning. However, the discovery
               of such CFDs has received limited attention. In this paper, we
               regard CFDs as an extension of association rules, and present
               three general methodologies for (approximate) CFD discovery,
               each using a different way of combining pattern mining for
               discovering the conditions (the ``C'' in CFD) with FD discovery.
               We discuss how existing algorithms fit into these three
               methodologies, and introduce new techniques to improve the
               discovery process. We show that the right choice of methodology
               improves performance over the traditional CFD discovery method
               CTane. Code related to this paper is available at:
               https://github.com/j-r77/cfddiscovery,
               https://codeocean.com/2018/06/20/discovering-conditional-functional-dependencies/code.",
  publisher = "Springer International Publishing",
  pages     = "552--568",
  year      =  2019
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Rammelaere2018-dh,
  title     = "Explaining repaired data with {CFDs}",
  author    = "Rammelaere, Joeri and Geerts, Floris",
  abstract  = "Many popular data cleaning approaches are rule-based:
               Constraints are formulated in a logical framework, and data is
               considered dirty if constraints are violated. These constraints
               are often discovered from data, but to ascertain their validity,
               user verification is necessary …",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  11,
  number    =  11,
  pages     = "1387--1399",
  year      =  2018
}

@ARTICLE{Kindi2019-vy,
  title     = "Data Civilizer 2.0",
  author    = "Kindi, Rezigel and {CaoLei} and {StonebrakerMichael} and
               {SimoniniGiovanni} and {TaoWenbo} and {MaddenSamuel} and
               {OuzzaniMourad} and {TangNan} and ElmagarmidAhmed, K",
  abstract  = "Data scientists spend over 80\% of their time (1)
               parameter-tuning machine learning models and (2) iterating
               between data cleaning and machine learning model execution.
               While there are existing effo...",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment PUB4722",
  month     =  aug,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Rahm2001-ei,
  title     = "A Survey of Approaches to Automatic Schema Matching",
  author    = "Rahm, Erhard and Bernstein, Philip A",
  journal   = "VLDB J.",
  publisher = "Springer-Verlag New York, Inc.",
  volume    =  10,
  number    =  4,
  pages     = "334--350",
  month     =  dec,
  year      =  2001,
  address   = "Secaucus, NJ, USA",
  keywords  = "Graph matching, Machine learning, Model management, Schema
               integration, Schema matching"
}

@INPROCEEDINGS{Heidari2019-ox,
  title     = "{HoloDetect}: {Few-Shot} Learning for Error Detection",
  booktitle = "Proceedings of the 2019 International Conference on Management
               of Data",
  author    = "Heidari, Alireza and McGrath, Joshua and Ilyas, Ihab F and
               Rekatsinas, Theodoros",
  publisher = "ACM",
  pages     = "829--846",
  month     =  jun,
  year      =  2019,
  keywords  = "data augmentation; error detection; few-shot learning; machine
               learning; weak supervision"
}

@ARTICLE{Boehm2019-ra,
  title         = "{SystemDS}: A Declarative Machine Learning System for the
                   {End-to-End} Data Science Lifecycle",
  author        = "Boehm, Matthias and Antonov, Iulian and Dokter, Mark and
                   Ginthoer, Robert and Innerebner, Kevin and Klezin, Florijan
                   and Lindstaedt, Stefanie and Phani, Arnab and Rath, Benjamin",
  abstract      = "Machine learning (ML) applications become increasingly
                   common in many domains. ML systems to execute these
                   workloads include numerical computing frameworks and
                   libraries, ML algorithm libraries, and specialized systems
                   for deep neural networks and distributed ML. These systems
                   focus primarily on efficient model training and scoring.
                   However, the data science process is exploratory, and deals
                   with underspecified objectives and a wide variety of
                   heterogeneous data sources. Therefore, additional tools are
                   employed for data engineering and debugging, which requires
                   boundary crossing, unnecessary manual effort, and lacks
                   optimization across the lifecycle. In this paper, we
                   introduce SystemDS, an open source ML system for the
                   end-to-end data science lifecycle from data integration,
                   cleaning, and preparation, over local, distributed, and
                   federated ML model training, to debugging and serving. To
                   this end, we aim to provide a stack of declarative languages
                   with R-like syntax for the different lifecycle tasks, and
                   users with different expertise. We describe the overall
                   system architecture, explain major design decisions
                   (motivated by lessons learned from Apache SystemML), and
                   discuss key features and research directions. Finally, we
                   provide preliminary results that show the potential of
                   end-to-end lifecycle optimization.",
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DB",
  eprint        = "1909.02976"
}

@ARTICLE{Mahdavi2019-mu,
  title   = "Towards Automated Data Cleaning Workflows",
  author  = "Mahdavi, Mohammad and Neutatz, Felix and Visengeriyeva, Larysa and
             Abedjan, Ziawasch",
  journal = "Mach. Learn.",
  volume  =  15,
  pages   = "16",
  year    =  2019
}

@ARTICLE{Cubuk2018-su,
  title         = "{AutoAugment}: Learning Augmentation Policies from Data",
  author        = "Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and
                   Vasudevan, Vijay and Le, Quoc V",
  abstract      = "Data augmentation is an effective technique for improving
                   the accuracy of modern image classifiers. However, current
                   data augmentation implementations are manually designed. In
                   this paper, we describe a simple procedure called
                   AutoAugment to automatically search for improved data
                   augmentation policies. In our implementation, we have
                   designed a search space where a policy consists of many
                   sub-policies, one of which is randomly chosen for each image
                   in each mini-batch. A sub-policy consists of two operations,
                   each operation being an image processing function such as
                   translation, rotation, or shearing, and the probabilities
                   and magnitudes with which the functions are applied. We use
                   a search algorithm to find the best policy such that the
                   neural network yields the highest validation accuracy on a
                   target dataset. Our method achieves state-of-the-art
                   accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without
                   additional data). On ImageNet, we attain a Top-1 accuracy of
                   83.5\% which is 0.4\% better than the previous record of
                   83.1\%. On CIFAR-10, we achieve an error rate of 1.5\%,
                   which is 0.6\% better than the previous state-of-the-art.
                   Augmentation policies we find are transferable between
                   datasets. The policy learned on ImageNet transfers well to
                   achieve significant improvements on other datasets, such as
                   Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft,
                   and Stanford Cars.",
  month         =  may,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1805.09501"
}

@ARTICLE{Ratner2017-tf,
  title         = "Learning to Compose {Domain-Specific} Transformations for
                   Data Augmentation",
  author        = "Ratner, Alexander J and Ehrenberg, Henry R and Hussain,
                   Zeshan and Dunnmon, Jared and R{\'e}, Christopher",
  abstract      = "Data augmentation is a ubiquitous technique for increasing
                   the size of labeled training sets by leveraging
                   task-specific data transformations that preserve class
                   labels. While it is often easy for domain experts to specify
                   individual transformations, constructing and tuning the more
                   sophisticated compositions typically needed to achieve
                   state-of-the-art results is a time-consuming manual task in
                   practice. We propose a method for automating this process by
                   learning a generative sequence model over user-specified
                   transformation functions using a generative adversarial
                   approach. Our method can make use of arbitrary,
                   non-deterministic transformation functions, is robust to
                   misspecified user input, and is trained on unlabeled data.
                   The learned transformation model can then be used to perform
                   data augmentation for any end discriminative model. In our
                   experiments, we show the efficacy of our approach on both
                   image and text datasets, achieving improvements of 4.0
                   accuracy points on CIFAR-10, 1.4 F1 points on the ACE
                   relation extraction task, and 3.4 accuracy points when using
                   domain-specific transformation operations on a medical
                   imaging dataset as compared to standard heuristic
                   augmentation approaches.",
  month         =  sep,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1709.01643"
}

@ARTICLE{Chu2013-qe,
  title     = "Discovering denial constraints",
  author    = "Chu, Xu and Ilyas, Ihab F and Papotti, Paolo",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  6,
  number    =  13,
  pages     = "1498--1509",
  month     =  aug,
  year      =  2013
}

@INPROCEEDINGS{Wang2019-jg,
  title     = "{Uni-Detect}: A Unified Approach to Automated Error Detection in
               Tables",
  booktitle = "Proceedings of the 2019 International Conference on Management
               of Data",
  author    = "Wang, Pei and He, Yeye",
  publisher = "ACM",
  pages     = "811--828",
  series    = "SIGMOD '19",
  year      =  2019,
  address   = "New York, NY, USA",
  keywords  = "constraints, data quality, error detection, outliers",
  location  = "Amsterdam, Netherlands"
}

@ARTICLE{Chu2016-il,
  title     = "Qualitative Data Cleaning",
  author    = "Chu, Xu and Ilyas, Ihab F",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  9,
  number    =  13,
  pages     = "1605--1608",
  month     =  sep,
  year      =  2016
}

@INPROCEEDINGS{Mahdavi2019-pk,
  title     = "{REDS}: Estimating the Performance of Error Detection Strategies
               Based on Dirtiness Profiles",
  booktitle = "Proceedings of the 31st International Conference on Scientific
               and Statistical Database Management",
  author    = "Mahdavi, Mohammad and Abedjan, Ziawasch",
  publisher = "ACM",
  pages     = "193--196",
  series    = "SSDBM '19",
  year      =  2019,
  address   = "New York, NY, USA",
  location  = "Santa Cruz, CA, USA"
}

@MISC{Yan_undated-wd,
  title  = "{CODED}: Column-oriented data error detection with statistical
            constraints",
  author = "Yan, Jing Nathan and Schulte, Oliver and Wang, Jiannan and Cheng,
            Reynold"
}

@ARTICLE{Rahman2018-wn,
  title     = "{ICARUS}: Minimizing Human Effort in Iterative Data Completion",
  author    = "Rahman, Protiva and Hebert, Courtney and Nandi, Arnab",
  abstract  = "An important step in data preparation involves dealing with
               incomplete datasets. In some cases, the missing values are
               unreported because they are characteristics of the domain and
               are known by practitioners. Due to this nature of the missing
               values, imputation and inference methods do not work and input
               from domain experts is required. A common method for experts to
               fill missing values is through rules. However, for large
               datasets with thousands of missing data points, it is laborious
               and time consuming for a user to make sense of the data and
               formulate effective completion rules. Thus, users need to be
               shown subsets of the data that will have the most impact in
               completing missing fields. Further, these subsets should provide
               the user with enough information to make an update. Choosing
               subsets that maximize the probability of filling in missing data
               from a large dataset is computationally expensive. To address
               these challenges, we present ICARUS, which uses a heuristic
               algorithm to show the user small subsets of the database in the
               form of a matrix. This allows the user to iteratively fill in
               data by applying suggested rules based on their direct edits to
               the matrix. The suggested rules amplify the users' input to
               multiple missing fields by using the database schema to infer
               hierarchies. Simulations show ICARUS has an average improvement
               of 50\% across three datasets over the baseline system. Further,
               in-person user studies demonstrate that naive users can fill in
               68\% of missing data within an hour, while manual rule
               specification spans weeks.",
  journal   = "Proceedings VLDB Endowment",
  publisher = "dl.acm.org",
  volume    =  11,
  number    =  13,
  pages     = "2263--2276",
  month     =  sep,
  year      =  2018,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Thirumuruganathan2017-ip,
  title     = "{UGuide}: {User-Guided} Discovery of {FD-Detectable} Errors",
  booktitle = "Proceedings of the 2017 {ACM} International Conference on
               Management of Data",
  author    = "Thirumuruganathan, Saravanan and Berti-Equille, Laure and
               Ouzzani, Mourad and Quiane-Ruiz, Jorge-Arnulfo and Tang, Nan",
  abstract  = "Error detection is the process of identifying problematic data
               cells that are different from their ground truth. Functional
               dependencies (FDs) have been widely studied in support of this
               process. Oftentimes, it is assumed that FDs are given by
               experts. Unfortunately, it is usually …",
  publisher = "ACM",
  pages     = "1385--1397",
  series    = "SIGMOD '17",
  year      =  2017,
  address   = "New York, NY, USA",
  keywords  = "budgeted error detection, functional dependencies, interactive
               error detection",
  location  = "Chicago, Illinois, USA"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Arocena2015-om,
  title     = "Messing Up with {BART}: Error Generation for Evaluating
               Data-cleaning Algorithms",
  author    = "Arocena, Patricia C and Glavic, Boris and Mecca, Giansalvatore
               and Miller, Ren{\'e}e J and Papotti, Paolo and Santoro,
               Donatello",
  abstract  = "We study the problem of introducing errors into clean databases
               for the purpose of benchmarking data-cleaning algorithms. Our
               goal is to provide users with the highest possible level of
               control over the error-generation process, and at the same time
               develop …",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  9,
  number    =  2,
  pages     = "36--47",
  month     =  oct,
  year      =  2015
}

@ARTICLE{Berti-Equille2018-ow,
  title     = "Discovery of Genuine Functional Dependencies from Relational
               Data with Missing Values",
  author    = "Berti-{\'E}quille, Laure and Harmouch, Hazar and Naumann, Felix
               and Novelli, No{\"e}l and Thirumuruganathan, Saravanan",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  11,
  number    =  8,
  pages     = "880--892",
  month     =  apr,
  year      =  2018
}

@INPROCEEDINGS{Visengeriyeva2018-qz,
  title     = "Metadata-driven Error Detection",
  booktitle = "Proceedings of the 30th International Conference on Scientific
               and Statistical Database Management",
  author    = "Visengeriyeva, Larysa and Abedjan, Ziawasch",
  publisher = "ACM",
  pages     = "1:1--1:12",
  series    = "SSDBM '18",
  year      =  2018,
  address   = "New York, NY, USA",
  keywords  = "error detection, systems aggregation",
  location  = "Bozen-Bolzano, Italy"
}

@INPROCEEDINGS{Neutatz2019-aw,
  title     = "{ED2}: A Case for Active Learning in Error Detection",
  booktitle = "Proceedings of the 28th {ACM} International Conference on
               Information and Knowledge Management",
  author    = "Neutatz, Felix and Mahdavi, Mohammad and Abedjan, Ziawasch",
  publisher = "ACM",
  pages     = "2249--2252",
  month     =  nov,
  year      =  2019,
  keywords  = "active learning; data quality; error detection; example-driven
               error detection"
}

@ARTICLE{Nosek2015-cc,
  title    = "{SCIENTIFIC} {STANDARDS}. Promoting an open research culture",
  author   = "Nosek, B A and Alter, G and Banks, G C and Borsboom, D and
              Bowman, S D and Breckler, S J and Buck, S and Chambers, C D and
              Chin, G and Christensen, G and Contestabile, M and Dafoe, A and
              Eich, E and Freese, J and Glennerster, R and Goroff, D and Green,
              D P and Hesse, B and Humphreys, M and Ishiyama, J and Karlan, D
              and Kraut, A and Lupia, A and Mabry, P and Madon, T A and
              Malhotra, N and Mayo-Wilson, E and McNutt, M and Miguel, E and
              Paluck, E Levy and Simonsohn, U and Soderberg, C and Spellman, B
              A and Turitto, J and VandenBos, G and Vazire, S and Wagenmakers,
              E J and Wilson, R and Yarkoni, T",
  journal  = "Science",
  volume   =  348,
  number   =  6242,
  pages    = "1422--1425",
  month    =  jun,
  year     =  2015,
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Suppes1964-et,
  title       = "What is a scientific theory?",
  author      = "Suppes, Patrick",
  abstract    = "Often when we ask what is a seand-so, pie expect a dear and
                 definite answer. If, for euample, someone asks me what is a
                 rational number, I may give the simple and precise armer that
                 a rational number is the ratio of two integen. There are other
                 kinds of simple …",
  publisher   = "pdfs.semanticscholar.org",
  institution = "US Information Agency, Voice of America Forum",
  year        =  1964
}

@INPROCEEDINGS{Mahdavi2019-mf,
  title     = "Towards Automated Data Cleaning Workflows",
  booktitle = "{LWDA} 2019",
  author    = "Mahdavi, Mohammad and Neutatz, Felix and Visengeriyeva, Larysa
               and Abedjan, Ziawasch",
  abstract  = "PDF | The success of AI-based technologies depends crucially on
               trustful and clean data. Research in data cleaning has provided
               a variety of approaches to address different data quality
               problems. Most of them require some prior knowledge about the
               dataset in order to select and...",
  month     =  sep,
  year      =  2019
}

@ARTICLE{Neutatz2019-yv,
  title         = "{ED2}: Two-stage Active Learning for Error Detection --
                   Technical Report",
  author        = "Neutatz, Felix and Mahdavi, Mohammad and Abedjan, Ziawasch",
  abstract      = "Traditional error detection approaches require user-defined
                   parameters and rules. Thus, the user has to know both the
                   error detection system and the data. However, we can also
                   formulate error detection as a semi-supervised
                   classification problem that only requires domain expertise.
                   The challenges for such an approach are twofold: (1) to
                   represent the data in a way that enables a classification
                   model to identify various kinds of data errors, and (2) to
                   pick the most promising data values for learning. In this
                   paper, we address these challenges with ED2, our new
                   example-driven error detection method. First, we present a
                   new two-dimensional multi-classifier sampling strategy for
                   active learning. Second, we propose novel multi-column
                   features. The combined application of these techniques
                   provides fast convergence of the classification task with
                   high detection accuracy. On several real-world datasets, ED2
                   requires, on average, less than 1\% labels to outperform
                   existing error detection approaches. This report extends the
                   peer-reviewed paper ``ED2: A Case for Active Learning in
                   Error Detection''. All source code related to this project
                   is available on GitHub.",
  month         =  aug,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1908.06309"
}

@INPROCEEDINGS{Castro_Fernandez2018-vf,
  title     = "Seeping Semantics: Linking Datasets Using Word Embeddings for
               Data Discovery",
  booktitle = "2018 {IEEE} 34th International Conference on Data Engineering
               ({ICDE})",
  author    = "Castro Fernandez, R and Mansour, E and Qahtan, A A and
               Elmagarmid, A and Ilyas, I and Madden, S and Ouzzani, M and
               Stonebraker, M and Tang, N",
  abstract  = "Employees that spend more time finding relevant data than
               analyzing it suffer from a data discovery problem. The large
               volume of data in enterprises, and sometimes the lack of
               knowledge of the schemas aggravates this problem. Similar to how
               we navigate the Web, we propose to identify semantic links that
               assist analysts in their discovery tasks. These links relate
               tables to each other, to facilitate navigating the schemas. They
               also relate data to external data sources, such as ontologies
               and dictionaries, to help explain the schema meaning. We
               materialize the links in an enterprise knowledge graph, where
               they become available to analysts. The main challenge is how to
               find pairs of objects that are semantically related. We propose
               SEMPROP, a DAG of different components that find links based on
               syntactic and semantic similarities. SEMPROP is commanded by a
               semantic matcher which leverages word embeddings to find objects
               that are semantically related. We introduce coherent group, a
               technique to combine word embeddings that works better than
               other state of the art combination alternatives. We implement
               SEMPROP as part of Aurum, a data discovery system we are
               building, and conduct user studies, real deployments and a
               quantitative evaluation to understand the benefits of links for
               data discovery tasks, as well as the benefits of SEMPROP and
               coherent groups to find those links.",
  pages     = "989--1000",
  month     =  apr,
  year      =  2018,
  keywords  = "business data processing;data mining;graph theory;ontologies
               (artificial intelligence);semantic Web;linking datasets;word
               embeddings;data discovery problem;semantic links;external data
               sources;schema meaning;enterprise knowledge
               graph;SEMPROP;semantic similarities;semantic matcher;data
               discovery system;data discovery
               tasks;Aurum;Semantics;Ontologies;Databases;Drugs;Task
               analysis;Syntactics;Proteins;data discovery;word embeddings"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Miller2018-lz,
  title     = "Open data integration",
  author    = "Miller, R J",
  abstract  = "Open data plays a major role in supporting both governmental and
               organizational transparency. Many organizations are adopting
               Open Data Principles promising to make their open data complete,
               primary, and timely. These properties make this data …",
  journal   = "Proceedings VLDB Endowment",
  publisher = "dl.acm.org",
  year      =  2018
}

@ARTICLE{Thirumuruganathan2018-sp,
  title   = "Data curation with deep learning [vision]: Towards self driving
             data curation",
  author  = "Thirumuruganathan, Saravanan and Tang, Nan and Ouzzani, Mourad",
  journal = "arXiv preprint arXiv:1803. 01384",
  year    =  2018
}

@ARTICLE{Miller2018-jt,
  title   = "Making Open Data Transparent: Data Discovery on Open Data",
  author  = "Miller, Ren{\'e}e J and Nargesian, Fatemeh and Zhu, Erkang and
             Christodoulakis, Christina and Pu, Ken Q and Andritsos, Periklis",
  journal = "IEEE Data Eng. Bull.",
  volume  =  41,
  number  =  2,
  pages   = "59--70",
  year    =  2018
}

@ARTICLE{Cichy2019-lq,
  title     = "An Overview of Data Quality Frameworks",
  author    = "Cichy, C and Rass, S",
  abstract  = "Nowadays, the importance of achieving and maintaining a high
               standard of data quality is widely recognized by both
               practitioners and researchers. Based on its impact on
               businesses, the quality of data is commonly viewed as a valuable
               asset. The literature comprises various techniques for defining,
               assessing, and improving data quality. However, requirements for
               data and their quality vary between organizations. Due to this
               variety, choosing suitable methods that are advantageous for the
               data quality of an organization or in a particular context can
               be challenging. This paper surveys data quality frameworks in a
               comparative way regarding the definition, assessment, and
               improvement of data quality with a focus on methodologies that
               are applicable in a wide range of business environments. To aid
               the decision process concerning the suitability of these
               methods, we further provide a decision guide to data quality
               frameworks. This guidance aims to help narrow down possible
               choices for data quality methodologies based on a number of
               specified criteria.",
  journal   = "IEEE Access",
  publisher = "ieeexplore.ieee.org",
  volume    =  7,
  pages     = "24634--24648",
  year      =  2019,
  keywords  = "business data processing;data analysis;data integrity;decision
               making;organisational aspects;data quality frameworks;data
               quality methodologies;business environments;decision
               guide;decision process;Data integrity;Standards
               organizations;Organizations;Decision making;Data warehouses;Data
               quality assessment;data structures;decision making;information
               management;quality management"
}

@ARTICLE{Pit--Claudel2016-dj,
  title     = "Outlier Detection in Heterogeneous Datasets using Automatic
               Tuple Expansion",
  author    = "Pit--Claudel, Cl{\'e}ment and Mariet, Zelda and Harding, Rachael
               and Madden, Sam",
  abstract  = "Rapidly developing areas of information technology are
               generating massive amounts of data. Human errors, sensor
               failures, and other unforeseen circumstances unfortunately tend
               to undermine the quality and consistency of these datasets by
               introducing outliers -- data points that exhibit surprising
               behavior when compared to the rest of the data. Characterizing,
               locating, and in some cases eliminating these outliers offers
               interesting insight about the data under scrutiny and reinforces
               the confidence that one may have in conclusions drawn from
               otherwise noisy datasets. In this paper, we describe a tuple
               expansion procedure which reconstructs rich information from
               semantically poor SQL data types such as strings, integers, and
               floating point numbers. We then use this procedure as the
               foundation of a new user-guided outlier detection framework,
               dBoost, which relies on inference and statistical modeling of
               heterogeneous data to flag suspicious fields in database tuples.
               We show that this novel approach achieves good classification
               performance, both in traditional numerical datasets and in
               highly non-numerical contexts such as mostly textual datasets.
               Our implementation is publicly available, under version 3 of the
               GNU General Public License.",
  publisher = "dspace.mit.edu",
  month     =  feb,
  year      =  2016
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Krishnan2016-rg,
  title     = "{ActiveClean}: An Interactive Data Cleaning Framework For Modern
               Machine Learning",
  booktitle = "Proceedings of the 2016 International Conference on Management
               of Data",
  author    = "Krishnan, Sanjay and Franklin, Michael J and Goldberg, Ken and
               Wang, Jiannan and Wu, Eugene",
  abstract  = "Databases can be corrupted with various errors such as missing,
               incorrect, or inconsistent values. Increasingly, modern data
               analysis pipelines involve Machine Learning, and the effects of
               dirty data can be difficult to debug. Dirty data is often
               sparse, and naive sampling …",
  publisher = "ACM",
  pages     = "2117--2120",
  series    = "SIGMOD '16",
  year      =  2016,
  address   = "New York, NY, USA",
  keywords  = "data cleaning, machine learning",
  location  = "San Francisco, California, USA"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Khayyat2015-kp,
  title     = "{BigDansing}: A System for Big Data Cleansing",
  booktitle = "Proceedings of the 2015 {ACM} {SIGMOD} International Conference
               on Management of Data",
  author    = "Khayyat, Zuhair and Ilyas, Ihab F and Jindal, Alekh and Madden,
               Samuel and Ouzzani, Mourad and Papotti, Paolo and
               Quian{\'e}-Ruiz, Jorge-Arnulfo and Tang, Nan and Yin, Si",
  abstract  = "Data cleansing approaches have usually focused on detecting and
               fixing errors with little attention to scaling to big datasets.
               This presents a serious impediment since data cleansing often
               involves costly computations such as enumerating pairs of
               tuples, handling inequality …",
  publisher = "ACM",
  pages     = "1215--1230",
  series    = "SIGMOD '15",
  year      =  2015,
  address   = "New York, NY, USA",
  keywords  = "cleansing abstraction, distributed data cleansing, distributed
               data repair, schema constraints",
  location  = "Melbourne, Victoria, Australia"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Krishnan2016-va,
  title     = "{ActiveClean}: Interactive Data Cleaning for Statistical
               Modeling",
  author    = "Krishnan, Sanjay and Wang, Jiannan and Wu, Eugene and Franklin,
               Michael J and Goldberg, Ken",
  abstract  = "Analysts often clean dirty data iteratively--cleaning some data,
               executing the analysis, and then cleaning more data based on the
               results. We explore the iterative cleaning process in the
               context of statistical model training, which is an increasingly
               popular form of data …",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  9,
  number    =  12,
  pages     = "948--959",
  month     =  aug,
  year      =  2016
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{He2016-gh,
  title     = "Interactive and Deterministic Data Cleaning",
  booktitle = "Proceedings of the 2016 International Conference on Management
               of Data",
  author    = "He, Jian and Veltri, Enzo and Santoro, Donatello and Li,
               Guoliang and Mecca, Giansalvatore and Papotti, Paolo and Tang,
               Nan",
  abstract  = "We present Falcon, an interactive, deterministic, and
               declarative data cleaning system, which uses SQL update queries
               as the language to repair data. Falcon does not rely on the
               existence of a set of pre-defined data quality rules. On the
               contrary, it encourages users to …",
  publisher = "ACM",
  pages     = "893--907",
  series    = "SIGMOD '16",
  year      =  2016,
  address   = "New York, NY, USA",
  keywords  = "data cleaning, declarative, deterministic, interactive",
  location  = "San Francisco, California, USA"
}

@INPROCEEDINGS{Abedjan2016-qp,
  title     = "{DataXFormer}: A robust transformation discovery system",
  booktitle = "2016 {IEEE} 32nd International Conference on Data Engineering
               ({ICDE})",
  author    = "Abedjan, Z and Morcos, J and Ilyas, I F and Ouzzani, M and
               Papotti, P and Stonebraker, M",
  abstract  = "In data integration, data curation, and other data analysis
               tasks, users spend a considerable amount of time converting data
               from one representation to another. For example US dates to
               European dates or airport codes to city names. In a previous
               vision paper, we presented the initial design of DataXFormer, a
               system that uses web resources to assist in transformation
               discovery. Specifically, DataXFormer discovers possible
               transformations from web tables and web forms and involves human
               feedback where appropriate. In this paper, we present the full
               fledged system along with several extensions. In particular, we
               present algorithms to find (i) transformations that entail
               multiple columns of input data, (ii) indirect transformations
               that are compositions of other transformations, (iii)
               transformations that are not functions but rather relationships,
               and (iv) transformations from a knowledge base of public data.
               We report on experiments with a collection of 120 transformation
               tasks, and show our enhanced system automatically covers 101 of
               them by using openly available resources.",
  publisher = "ieeexplore.ieee.org",
  pages     = "1134--1145",
  month     =  may,
  year      =  2016,
  keywords  = "Urban areas;Airports;Knowledge based systems;Search
               engines;Computer architecture;Semantics;Indexing"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Chung2017-cl,
  title     = "A data quality metric ({DQM)}: how to estimate the number of
               undetected errors in data sets",
  author    = "Chung, Y and Krishnan, S and Kraska, T",
  abstract  = "Data cleaning, whether manual or algorithmic, is rarely perfect
               leaving a dataset with an unknown number of false positives and
               false negatives after cleaning. In many scenarios, quantifying
               the number of remaining errors is challenging because our data
               integrity rules …",
  journal   = "Proceedings VLDB Endowment",
  publisher = "dl.acm.org",
  year      =  2017
}

@INPROCEEDINGS{Chuck2017-vb,
  title     = "Statistical data cleaning for deep learning of automation tasks
               from demonstrations",
  booktitle = "2017 13th {IEEE} Conference on Automation Science and
               Engineering ({CASE})",
  author    = "Chuck, C and Laskey, M and Krishnan, S and Joshi, R and Fox, R
               and Goldberg, K",
  abstract  = "Automation using deep learning from demonstrations requires many
               training examples. Gathering this data is time consuming and
               expensive, and human demonstrators are prone to inconsistencies
               and errors that can delay or degrade learning. This paper
               explores how characterizing supervisor inconsistency and
               correcting for this noise can improve task performance with a
               limited budget of data. We consider a planar part extraction
               task (separating one part from a group) where human operators
               provide demonstrations by teleoperating a 2DOF robot. We analyze
               30, 000 image-control pairs from 480 trajectories. After error
               corrections, trained CNN models show an improvement of 11.2\%
               upon the baseline in mean absolute success rate.",
  pages     = "1142--1149",
  month     =  aug,
  year      =  2017,
  keywords  = "automatic programming;data handling;learning (artificial
               intelligence);neural nets;telerobotics;training examples;human
               demonstrators;task performance;planar part extraction task;human
               operators;error corrections;trained CNN models;statistical data
               cleaning;deep learning;automation tasks;supervisor
               inconsistency;2DOF robot;image-control pairs;mean absolue
               success rate;Cleaning;Trajectory;Robot sensing
               systems;Jitter;Neural networks"
}

@ARTICLE{Schelter2018-au,
  title     = "Automating Large-scale Data Quality Verification",
  author    = "Schelter, Sebastian and Lange, Dustin and Schmidt, Philipp and
               Celikel, Meltem and Biessmann, Felix and Grafberger, Andreas",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  11,
  number    =  12,
  pages     = "1781--1794",
  month     =  aug,
  year      =  2018
}

@INPROCEEDINGS{Qahtan2018-te,
  title     = "{FAHES}: A Robust Disguised Missing Values Detector",
  booktitle = "Proceedings of the 24th {ACM} {SIGKDD} International Conference
               on Knowledge Discovery \& Data Mining",
  author    = "Qahtan, Abdulhakim A and Elmagarmid, Ahmed and Castro Fernandez,
               Raul and Ouzzani, Mourad and Tang, Nan",
  publisher = "ACM",
  pages     = "2100--2109",
  series    = "KDD '18",
  year      =  2018,
  address   = "New York, NY, USA",
  keywords  = "disguised missing value, numerical outliers, syntactic outliers,
               syntactic patterns",
  location  = "London, United Kingdom"
}

@INPROCEEDINGS{Huang2018-er,
  title     = "{Auto-Detect}: {Data-Driven} Error Detection in Tables",
  booktitle = "Proceedings of the 2018 International Conference on Management
               of Data",
  author    = "Huang, Zhipeng and He, Yeye",
  publisher = "ACM",
  pages     = "1377--1392",
  series    = "SIGMOD '18",
  year      =  2018,
  address   = "New York, NY, USA",
  keywords  = "data cleaning, data-driven error detection, outliers, table
               corpus",
  location  = "Houston, TX, USA"
}

@ARTICLE{Stonebraker2018-ag,
  title   = "Data Integration: The Current Status and the Way Forward",
  author  = "Stonebraker, Michael and Ilyas, Ihab F",
  journal = "IEEE Data Eng. Bull.",
  volume  =  41,
  number  =  2,
  pages   = "3--9",
  year    =  2018
}

@INPROCEEDINGS{Deng2017-xp,
  title     = "The Data Civilizer System",
  booktitle = "Cidr",
  author    = "Deng, Dong and Fernandez, Raul Castro and Abedjan, Ziawasch and
               Wang, Sibo and Stonebraker, Michael and Elmagarmid, Ahmed K and
               Ilyas, Ihab F and Madden, Samuel and Ouzzani, Mourad and Tang,
               Nan",
  year      =  2017
}

@ARTICLE{Singh2017-an,
  title     = "Synthesizing Entity Matching Rules by Examples",
  author    = "Singh, Rohit and Meduri, Venkata Vamsikrishna and Elmagarmid,
               Ahmed and Madden, Samuel and Papotti, Paolo and Quian{\'e}-Ruiz,
               Jorge-Arnulfo and Solar-Lezama, Armando and Tang, Nan",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  11,
  number    =  2,
  pages     = "189--202",
  month     =  oct,
  year      =  2017
}

@ARTICLE{Rammelaere2019-ea,
  title    = "Cleaning Data with Forbidden Itemsets",
  author   = "Rammelaere, J and Geerts, F",
  abstract = "Methods for cleaning dirty data typically employ additional
              information about the data, such as user-provided constraints
              specifying when data is dirty, e.g., domain restrictions, illegal
              value combinations, or logical rules. However, real-world
              scenarios usually only have dirty data available, without known
              constraints. In such settings, constraints are automatically
              discovered on dirty data and discovered constraints are used to
              detect and repair errors. Typical repairing processes stop there.
              Yet when constraint discovery algorithms are re-run on the
              repaired data (assumed to be clean), new constraints and thus
              errors are often found. The repairing process thus introduces new
              constraint violations. We present a different type of repairing
              method, which prevents introducing new constraint violations,
              according to a discovery algorithm. Summarily, our repairs
              guarantee that all errors identified by constraints discovered on
              the dirty data are fixed; and the constraint discovery process
              cannot identify new constraint violations. We do this for a new
              kind of constraints, called forbidden itemsets (FBIs), capturing
              unlikely value co-occurrences. We show that FBIs detect errors
              with high precision. Evaluation on real-world data shows that our
              repair method obtains high-quality repairs without introducing
              new FBIs. Optional user interaction is readily integrated, with
              users deciding how much effort to invest.",
  journal  = "IEEE Trans. Knowl. Data Eng.",
  pages    = "1--1",
  year     =  2019,
  keywords = "Itemsets;Maintenance engineering;Cleaning;Data integrity;Data
              mining;Heuristic algorithms;Data cleaning;error detection;itemset
              mining"
}

@ARTICLE{Li2019-ve,
  title         = "{CleanML}: A Benchmark for Joint Data Cleaning and Machine
                   Learning [Experiments and Analysis]",
  author        = "Li, Peng and Rao, Xi and Blase, Jennifer and Zhang, Yue and
                   Chu, Xu and Zhang, Ce",
  abstract      = "It is widely recognized that the data quality affects
                   machine learning (ML) model performances, and data
                   scientists spend considerable amount of time on data
                   cleaning before model training. However, to date, there does
                   not exist a rigorous study on how exactly does cleaning
                   affect ML --- ML community usually focuses on the effects of
                   specific types of noises of certain distributions (e.g.,
                   mislabels) on certain ML models, while database (DB)
                   community has been mostly studying the problem of data
                   cleaning alone without considering how data is consumed by
                   downstream analytics. We propose the CleanML benchmark that
                   systematically investigates the impact of data cleaning on
                   downstream ML models. The CleanML benchmark currently
                   includes 13 real-world datasets with real errors, five
                   common error types, and seven different ML models. To ensure
                   that our findings are statistically significant, CleanML
                   carefully controls the randomness in ML experiments using
                   statistical hypothesis testing, and also uses the
                   Benjamini-Yekutieli (BY) procedure to control potential
                   false discoveries due to many hypotheses in the benchmark.
                   We obtain many interesting and non-trivial insights, and
                   identify multiple open research directions. We also release
                   the benchmark and hope to invite future studies on the
                   important problems of joint data cleaning and ML.",
  month         =  apr,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DB",
  eprint        = "1904.09483"
}

@INPROCEEDINGS{Mahdavi2019-zf,
  title     = "Raha: A {Configuration-Free} Error Detection System",
  booktitle = "Proceedings of the 2019 International Conference on Management
               of Data",
  author    = "Mahdavi, Mohammad and Abedjan, Ziawasch and Castro Fernandez,
               Raul and Madden, Samuel and Ouzzani, Mourad and Stonebraker,
               Michael and Tang, Nan",
  publisher = "ACM",
  pages     = "865--882",
  series    = "SIGMOD '19",
  year      =  2019,
  address   = "New York, NY, USA",
  keywords  = "classification, clustering, data cleaning, error detection,
               historical data, label propagation, machine learning,
               semi-supervised learning",
  location  = "Amsterdam, Netherlands"
}

@ARTICLE{Rekatsinas2017-iw,
  title     = "{HoloClean}: Holistic Data Repairs with Probabilistic Inference",
  author    = "Rekatsinas, Theodoros and Chu, Xu and Ilyas, Ihab F and R{\'e},
               Christopher",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  10,
  number    =  11,
  pages     = "1190--1201",
  month     =  aug,
  year      =  2017
}

@ARTICLE{Jin2015-oq,
  title    = "Significance and Challenges of Big Data Research",
  author   = "Jin, Xiaolong and Wah, Benjamin W and Cheng, Xueqi and Wang,
              Yuanzhuo",
  abstract = "In recent years, the rapid development of Internet, Internet of
              Things, and Cloud Computing have led to the explosive growth of
              data in almost every industry and business area. Big data has
              rapidly developed into a hot topic that attracts extensive
              attention from academia, industry, and governments around the
              world. In this position paper, we first briefly introduce the
              concept of big data, including its definition, features, and
              value. We then identify from different perspectives the
              significance and opportunities that big data brings to us. Next,
              we present representative big data initiatives all over the
              world. We describe the grand challenges (namely, data complexity,
              computational complexity, and system complexity), as well as
              possible solutions to address these challenges. Finally, we
              conclude the paper by presenting several suggestions on carrying
              out big data projects.",
  journal  = "Big Data Research",
  volume   =  2,
  number   =  2,
  pages    = "59--64",
  month    =  jun,
  year     =  2015,
  keywords = "Big data; Data complexity; Computational complexity; System
              complexity"
}

@ARTICLE{Cai2015-hr,
  title     = "The Challenges of Data Quality and Data Quality Assessment in
               the Big Data Era",
  author    = "Cai, Li and Zhu, Yangyong",
  abstract  = "High-quality data are the precondition for analyzing and using
               big data and for guaranteeing the value of the data. Currently,
               comprehensive analysis and research of quality standards and
               quality assessment methods for big data are lacking. First, this
               paper summarizes reviews of data quality research. Second, this
               paper analyzes the data characteristics of the big data
               environment, presents quality challenges faced by big data, and
               formulates a hierarchical data quality framework from the
               perspective of data users. This framework consists of big data
               quality dimensions, quality characteristics, and quality
               indexes. Finally, on the basis of this framework, this paper
               constructs a dynamic assessment process for data quality. This
               process has good expansibility and adaptability and can meet the
               needs of big data quality assessment. The research results
               enrich the theoretical scope of big data and lay a solid
               foundation for the future by establishing an assessment model
               and studying evaluation algorithms.",
  journal   = "Data Science Journal",
  publisher = "Ubiquity Press",
  volume    =  14,
  number    =  0,
  pages     = "2",
  month     =  may,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Abedjan2016-jc,
  title     = "Detecting Data Errors: Where Are We and What Needs to Be Done?",
  author    = "Abedjan, Ziawasch and Chu, Xu and Deng, Dong and Fernandez, Raul
               Castro and Ilyas, Ihab F and Ouzzani, Mourad and Papotti, Paolo
               and Stonebraker, Michael and Tang, Nan",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  9,
  number    =  12,
  pages     = "993--1004",
  month     =  aug,
  year      =  2016
}

@ARTICLE{Maletic2000-hl,
  title    = "Data Cleansing: Beyond Integrity Analysis",
  author   = "Maletic, J I and Marcus, Andrian",
  abstract = "The paper analyzes the problem of data cleansing and
              automatically identifying potential errors in data sets. An
              overview of the diminutive amount of existing literature
              concerning data cleansing is given. Methods for error detection
              that go beyond integrity analysis are reviewed and presented. The
              applicable methods include: statistical outlier detection,
              pattern matching, clustering, and data mining techniques. Some
              brief results supporting the use of such methods are given. The
              future research directions necessary to address the data
              cleansing problem are discussed.",
  journal  = "Iq",
  pages    = "1--10",
  year     =  2000,
  keywords = "data cleaning; data cleansing; data quality; error detection"
}

@INPROCEEDINGS{Chu2016-bg,
  title     = "Data cleaning: Overview and emerging challenges",
  booktitle = "Proceedings of the {ACM} {SIGMOD} International Conference on
               Management of Data",
  author    = "Chu, Xu and Ilyas, Ihab F and Krishnan, Sanjay and Wang, Jiannan",
  abstract  = "Detecting and repairing dirty data is one of the perennial
               challenges in data analytics, and failure to do so can result in
               inaccurate analytics and unreliable decisions. Over the past few
               years, there has been a surge of interest from both industry and
               academia on data cleaning problems including new abstractions,
               interfaces, approaches for scalability, and statistical
               techniques. To better understand the new advances in the field,
               we will first present a taxonomy of the data cleaning literature
               in which we highlight the recent interest in techniques that use
               constraints, rules, or patterns to detect errors, which we call
               qualitative data cleaning. We will describe the state-of-the-art
               techniques and also highlight their limitations with a series of
               illustrative examples. While traditionally such approaches are
               distinct from quantitative approaches such as outlier detection,
               we also discuss recent work that casts such approaches into a
               statistical estimation framework including: using Machine
               Learning to improve the efficiency and accuracy of data cleaning
               and considering the effects of data cleaning on statistical
               analysis.",
  publisher = "Association for Computing Machinery",
  volume    = "26-June-20",
  pages     = "2201--2206",
  month     =  jun,
  year      =  2016
}

@ARTICLE{Ilyas2015-oh,
  title     = "Trends in Cleaning Relational Data: Consistency and
               Deduplication",
  author    = "Ilyas, Ihab F and Chu, Xu",
  abstract  = "Data quality is one of the most important problems in data
               management, since dirty data often leads to inaccurate data
               analytics results and wrong business decisions. Poor data across
               businesses and the government cost the U.S. economy \$3.1
               trillion a year, according to a report by InsightSquared in
               2012. To detect data errors, data quality rules or integrity
               constraints (ICs) have been proposed as a declarative way to
               describe legal or correct data instances. Any subset of data
               that does not conform to the defined rules is considered
               erroneous, which is also referred to as a violation. Various
               kinds of data repairing techniques with different objectives
               have been introduced, where algorithms are used to detect
               subsets of the data that violate the declared integrity
               constraints, and even to suggest updates to the database such
               that the new database instance conforms with these constraints.
               While some of these algorithms aim to minimally change the
               database, others involve human experts or knowledge bases to
               verify the repairs suggested by the automatic repeating
               algorithms. In this paper, we discuss the main facets and
               directions in designing error detection and repairing
               techniques. We propose a taxonomy of current anomaly detection
               techniques, including error types, the automation of the
               detection process, and error propagation. We also propose a
               taxonomy of current data repairing techniques, including the
               repair target, the automation of the repair process, and the
               update model. We conclude by highlighting current trends in
               ``big data'' cleaning. \copyright{} 2015 I. F. Ilyas and X. Chu.",
  journal   = "Foundations and Trends\textregistered{} in Databases",
  publisher = "Now Publishers",
  volume    =  5,
  number    =  4,
  pages     = "281--393",
  year      =  2015,
  keywords  = "Data Cleaning and Information Extraction; Data Integration and
               Exchange"
}

@ARTICLE{Ebaid2013-yq,
  title     = "{NADEEF}: A generalized data cleaning system",
  author    = "Ebaid, Amr and Elmagarmid, Ahmed and Ilyas, Ihab F and Ouzzani,
               Mourad and Quiane-Ruiz, Jorge Arnulfo and Tang, Nan and Yin, Si",
  abstract  = "We present NADEEF, an extensible, generic and easy-to-deploy
               data cleaning system. NADEEF distinguishes between a programming
               interface and a core to achieve generality and extensibility.
               The programming interface allows users to specify data quality
               rules by writing code that implements predefined classes. These
               classes uniformly define what is wrong with the data and
               (possibly) how to fix it. We will demonstrate the following
               features provided by NADEEF. (1) Heterogeneity: The programming
               interface can be used to express many types of data quality
               rules beyond the well known CFDs (FDs), MDs and ETL rules. (2)
               Interdependency: The core algorithms can interleave multiple
               types of rules to detect and repair data errors. (3) Deployment
               and extensibility: Users can easily customize NADEEF by defining
               new types of rules, or by extending the core. (4) Metadata
               management and data custodians: We show a live data quality
               dashboard to effectively involve users in the data cleaning
               process. \copyright{} 2013 VLDB Endowment.",
  journal   = "Proceedings VLDB Endowment",
  publisher = "Association for Computing Machinery",
  volume    =  6,
  number    =  12,
  pages     = "1218--1221",
  year      =  2013
}

@INPROCEEDINGS{Tang2014-xj,
  title     = "Big data cleaning",
  booktitle = "Lecture Notes in Computer Science (including subseries Lecture
               Notes in Artificial Intelligence and Lecture Notes in
               Bioinformatics)",
  author    = "Tang, Nan",
  abstract  = "Data cleaning is, in fact, a lively subject that has played an
               important part in the history of data management and data
               analytics, and it still is undergoing rapid development.
               Moreover, data cleaning is considered as a main challenge in the
               era of big data, due to the increasing volume, velocity and
               variety of data in many applications. This paper aims to provide
               an overview of recent work in different aspects of data
               cleaning: error detection methods, data repairing algorithms,
               and a generalized data cleaning system. It also includes some
               discussion about our efforts of data cleaning methods from the
               perspective of big data, in terms of volume, velocity and
               variety.",
  publisher = "Springer Verlag",
  volume    = "8709 LNCS",
  pages     = "13--24",
  year      =  2014
}

@ARTICLE{Liu2004-kf,
  title    = "On-line outlier detection and data cleaning",
  author   = "Liu, Hancong and Shah, Sirish and Jiang, Wei",
  abstract = "Outliers are observations that do not follow the statistical
              distribution of the bulk of the data, and consequently may lead
              to erroneous results with respect to statistical analysis. Many
              conventional outlier detection tools are based on the assumption
              that the data is identically and independently distributed. In
              this paper, an outlier-resistant data filter-cleaner is proposed.
              The proposed data filter-cleaner includes an on-line
              outlier-resistant estimate of the process model and combines it
              with a modified Kalman filter to detect and ``clean'' outliers.
              The advantage over existing methods is that the proposed method
              has the following features: (a) a priori knowledge of the process
              model is not required; (b) it is applicable to autocorrelated
              data; (c) it can be implemented on-line; and (d) it tries to only
              clean (i.e., detects and replaces) outliers and preserves all
              other information in the data. \copyright{} 2004 Elsevier Ltd.
              All rights reserved.",
  journal  = "Comput. Chem. Eng.",
  volume   =  28,
  number   =  9,
  pages    = "1635--1647",
  month    =  aug,
  year     =  2004,
  keywords = "Breakdown point; Data filter-cleaner; Data preprocessing; Outlier
              detection; Time series analysis"
}

@MISC{Van_Den_Broeck2005-cb,
  title    = "Data cleaning: Detecting, diagnosing, and editing data
              abnormalities",
  author   = "Van Den Broeck, Jan and Cunningham, Solveig Argeseanu and
              Eeckels, Roger and Herbst, Kobus",
  abstract = "In this policy forum the authors argue that data cleaning is an
              essential part of the research process, and should be
              incorporated into study design.",
  journal  = "PLoS Medicine",
  volume   =  2,
  number   =  10,
  pages    = "0966--0970",
  year     =  2005
}

@INPROCEEDINGS{Ali2010-fz,
  title     = "A framework to implement Data Cleaning in Enterprise Data
               Warehouse for Robust Data Quality",
  booktitle = "2010 International Conference on Information and Emerging
               Technologies, {ICIET} 2010",
  author    = "Ali, Kamran and Warraich, Mubeen Ahmed",
  abstract  = "Every day, every hour, every minute, every second trillion of
               bytes of data is being generated by enterprises especially in
               telecom sector. To achieve level best decisions for business
               profits, access to that data in a well-situated and interactive
               way is always a dream of business executives and managers. Data
               warehouse is the only viable solution that can bring that dream
               into a reality. The enhancement of future endeavors to make
               decisions depends on the availability of correct information
               that based on quality of data underlying. The quality data can
               only be produced by cleaning data prior to loading into data
               warehouse. So correctness of data is essential for well-informed
               and reliable decision making. The framework proposed in this
               paper implements robust data quality to ensure consistent and
               correct loading of data into data warehouse that necessary to
               disciplined, accurate and reliable data analysis, data mining
               and knowledge discovery.",
  year      =  2010,
  keywords  = "Data cleaning; Data quality; Data warehousing and data mining"
}

@ARTICLE{Kavitha_Kumar2011-ko,
  title    = "Attribute Correction - Data Cleaning Using Association Rule and
              Clustering Methods",
  author   = "Kavitha Kumar, R and Chadrasekaran, R M",
  abstract = "Data cleaning, also called data cleansing or scrubbing, deals
              with detecting and removing errors and inconsistencies from data
              in order to improve the quality of data. Data quality problems
              are present in single data collections, such as files and
              databases,. When multiple data sources need to be integrated,
              e.g., in data warehouses, federated database systems or global
              web-based information systems, the need for data cleaning
              increases significantly. Data cleaning is the necessary condition
              of knowledge discovery and data warehouse building. In this paper
              two algorithms are designed using data mining technique to
              correct the attribute without external reference. One is
              Context-dependent attribute correction and another is
              Context-independent attribute correction.",
  journal  = "International Journal of Data Mining \& Knowledge Management
              Process",
  year     =  2011
}

@ARTICLE{Gill2014-gt,
  title    = "A Review of Contemporary Data Quality Issues in Data Warehouse
              {ETL} Environment",
  author   = "Gill, Rupali and Singh, Jaiteg",
  abstract = "In today's scenario, extraction--transformation-- loading (eTl)
              tools have become important pieces of software responsible for
              integrating heterogeneous information from several sources. The
              task of carrying out the eTl process is potentially a complex,
              hard and time consuming. Organisations now --a-days are concerned
              about vast qualities of data. The data quality is concerned with
              technical issues in data warehouse environment. Research in last
              few decades has laid more stress on data quality issues in a data
              warehouse eTl process. The data quality can be ensured cleaning
              the data prior to loading the data into a warehouse. Since the
              data is collected from various sources, it comes in various
              formats. The standardization of formats and cleaning such data
              becomes the need of clean data warehouse environment. Data
              quality attributes like accuracy, correctness, consistency,
              timeliness are required for a Knowledge discovery process. The
              present state -of --the-art purpose of the research work is to
              deal on data quality issues at all the aforementioned stages of
              data warehousing 1) Data sources, 2) Data integration 3) Data
              staging, 4) Data warehouse modelling and schematic design and to
              formulate descriptive classification of these causes. The
              discovered knowledge is used to repair the data deficiencies.
              This work proposes a framework for quality of extraction
              transformation and loading of data into a warehouse.",
  journal  = "Journal on Today's Ideas - Tomorrow's Technologies",
  volume   =  2,
  number   =  2,
  pages    = "153--160",
  year     =  2014
}

@INPROCEEDINGS{Dallachiesat2013-on,
  title     = "{NADEEF}: A commodity data cleaning system",
  booktitle = "Proceedings of the {ACM} {SIGMOD} International Conference on
               Management of Data",
  author    = "Dallachiesat, Michele and Ebaid, Amr and Eldawy, Ahmed and
               Elmagarmid, Ahmed and Ilyas, Ihab F and Ouzzani, Mourad and
               Tang, Nan",
  abstract  = "Despite the increasing importance of data quality and the rich
               theoretical and practical contributions in all aspects of data
               cleaning, there is no single end-to-end off-the-shelf solution
               to (semi-)automate the detection and the repairing of violations
               w.r.t. a set of heterogeneous and ad-hoc quality constraints. In
               short, there is no commodity platform similar to general purpose
               DBMSs that can be easily customized and deployed to solve
               application-specific data quality problems. In this paper, we
               present NADEEF, an extensible, generalized and easy-to-deploy
               data cleaning platform. NADEEF distinguishes between a
               programming interface and a core to achieve generality and
               extensibility. The programming interface allows the users to
               specify multiple types of data quality rules, which uniformly
               define what is wrong with the data and (possibly) how to repair
               it through writing code that implements predefined classes. We
               show that the programming interface can be used to express many
               types of data quality rules beyond the well known CFDs (FDs),
               MDs and ETL rules. Treating user implemented interfaces as
               black-boxes, the core provides algorithms to detect errors and
               to clean data. The core is designed in a way to allow cleaning
               algorithms to cope with multiple rules holistically, i.e.,
               detecting and repairing data errors without differentiating
               between various types of rules. We showcase two implementations
               for core repairing algorithms. These two implementations
               demonstrate the extensibility of our core, which can also be
               replaced by other user-provided algorithms. Using real-life
               data, we experimentally verify the generality, extensibility,
               and effectiveness of our system. Copyright \copyright{} 2013
               ACM.",
  pages     = "541--552",
  year      =  2013,
  keywords  = "Conditional functional dependency; Data cleaning; ETL; Matching
               dependency"
}

@MISC{Hodge2004-ii,
  title    = "A survey of outlier detection methodologies",
  author   = "Hodge, Victoria J and Austin, Jim",
  journal  = "Artificial Intelligence Review",
  year     =  2004,
  keywords = "Anomaly; Detection; Deviation; Noise; Novelty; Outlier;
              Recognition"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Hamming1950-uv,
  title    = "Error detecting and error correcting codes",
  author   = "Hamming, R W",
  abstract = "The author was led to the study given in this paper from a
              consideration of large scale computing machines in which a large
              number of operations must be performed without a single error in
              the end result. This problem of �doing things right� on a large
              scale is not essentially new; in a telephone central office, for
              example, a very large number of operations are performed while
              the errors leading to wrong numbers are kept well under control,
              though they have not been completely eliminated. This has been
              achieved, in part, through the use of self-checking circuits. The
              occasional failure that escapes routine checking is still
              detected by the customer and will, if it persists, result in
              customer complaint, while if it is transient it will produce only
              occasional wrong numbers. At the same time the rest of the
              central office functions satisfactorily. In a digital computer,
              on the other hand, a single failure usually means the complete
              failure, in the sense that if it is detected no more computing
              can be done until the failure is located and corrected, while if
              it escapes detection then it invalidates all subsequent
              operations of the machine. Put in other words, in a telephone
              central office there are a number of parallel paths which are
              more or less independent of each other; in a digital machine
              there is usually a single long path which passes through the same
              piece of equipment many, many times before the answer is
              obtained.",
  journal  = "The Bell System Technical Journal",
  volume   =  29,
  number   =  2,
  pages    = "147--160",
  month    =  apr,
  year     =  1950
}

@INPROCEEDINGS{Cattral2001-vn,
  title     = "Supervised and unsupervised data mining with an evolutionary
               algorithm",
  booktitle = "Proceedings of the {IEEE} Conference on Evolutionary
               Computation, {ICEC}",
  author    = "Cattral, R and Oppacher, F and Deugo, D",
  publisher = "IEEE",
  volume    =  2,
  pages     = "767--774",
  year      =  2001
}

@INPROCEEDINGS{Kiran2018-oh,
  title     = "A Comprehensive Survey on Privacy Preservation Algorithms in
               Data Mining",
  booktitle = "2017 {IEEE} International Conference on Computational
               Intelligence and Computing Research, {ICCIC} 2017",
  author    = "Kiran, Ajmeera and Vasumathi, D",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  month     =  nov,
  year      =  2018,
  keywords  = "Association; Classification Method; Clustering Technique; Data
               Distortion; Data mining; K-Anonymity; Privacy Preserving;
               Randomization; Secure Multiparty Computation"
}

@INPROCEEDINGS{Cavaretta1999-rt,
  title     = "Data mining using genetic programming: The implications of
               parsimony on generalization error",
  booktitle = "Proceedings of the 1999 Congress on Evolutionary Computation,
               {CEC} 1999",
  author    = "Cavaretta, Michael J and Chellapilla, Kumar",
  abstract  = "mcavaret @ ford.com Abstract- A common data mining heuristic is,
               ``When choosing between models with the same training error,
               less complex models should be preferred as they perform better
               on unseen data. '' This heuristic may not always hold. In
               genetic programming a preference for less complex models is
               implemented as i) placing a limit on the size of the evolved
               program ii) penalizing more complex individuals, or both. This
               paper presents a GP-variant with no limit on the complexity of
               the evolved program that generates highly accurate models on a
               common dataset. 1.",
  publisher = "IEEE Computer Society",
  volume    =  2,
  pages     = "1330--1337",
  year      =  1999
}

@INPROCEEDINGS{Chaudhuri2006-xi,
  title     = "A primitive operator for similarity joins in data cleaning",
  booktitle = "Proceedings - International Conference on Data Engineering",
  author    = "Chaudhuri, Surajit and Ganti, Venkatesh and Kaushik, Raghav",
  abstract  = "Data cleaning based on similarities involves identification of
               ``close'' tuples, where closeness is evaluated using a variety
               of similarity functions chosen to suit the domain and
               application. Current approaches for efficiently implementing
               such similarity joins are tightly tied to the chosen similarity
               function. In this paper, we propose a new primitive operator
               which can be used as a foundation to implement similarity joins
               according to a variety of popular string similarity functions,
               and notions of similarity which go beyond textual similarity. We
               then propose efficient implementations for this operator. In an
               experimental evaluation using real datasets, we show that the
               implementation of similarity joins using our operator is
               comparable to, and often substantially better than, previous
               customized implementations for particular similarity functions.",
  volume    =  2006,
  pages     = "5",
  year      =  2006
}

@INPROCEEDINGS{Mayfield2010-ag,
  title     = "{ERACER}: A database approach for statistical inference and data
               cleaning",
  booktitle = "Proceedings of the {ACM} {SIGMOD} International Conference on
               Management of Data",
  author    = "Mayfield, Chris and Neville, Jennifer and Prabhakar, Sunil",
  abstract  = "Real-world databases often contain syntactic and semantic
               errors, in spite of integrity constraints and other safety
               measures incorporated into modern DBMSs. We present ERACER, an
               iterative statistical framework for inferring missing
               information and correcting such errors automatically. Our
               approach is based on belief propagation and relational
               dependency networks, and includes an efficient approximate
               inference algorithm that is easily implemented in standard DBMSs
               using SQL and user defined functions. The system performs the
               inference and cleansing tasks in an integrated manner, using
               shrinkage techniques to infer correct values accurately even in
               the presence of dirty data. We evaluate the proposed methods
               empirically on multiple synthetic and real-world data sets. The
               results show that our framework achieves accuracy comparable to
               a baseline statistical method using Bayesian networks with exact
               inference. However, our framework has wider applicability than
               the Bayesian network baseline, due to its ability to reason with
               complex, cyclic relational dependencies.",
  pages     = "75--86",
  year      =  2010,
  keywords  = "approximate inference; discrete convolution; linear regression;
               outlier detection; relational dependency network"
}

@ARTICLE{Cafarella2009-uo,
  title     = "Data integration for the relational web",
  author    = "Cafarella, Michael J and Halevy, Alon and Khoussainova, Nodira",
  abstract  = "The Web contains a vast amount of structured information such as
               HTML tables, HTML lists and deep-web databases; there is
               enormous potential in combining and re-purposing this data in
               creative ways. However, integrating data from this relational
               web raises several challenges that are not addressed by current
               data integration systems or mash-up tools. First, the structured
               data is usually not published cleanly and must be extracted
               (say, from an HTML list) before it can be used. Second, due to
               the vastness of the corpus, a user can never know all of the
               potentially-relevant databases ahead of time (much less write a
               wrapper or mapping for each one); the source databases must be
               discovered during the integration process. Third, some of the
               important information regarding the data is only present in its
               enclosing web page and needs to be extracted appropriately. This
               paper describes Octopus, a system that combines search,
               extraction, data cleaning and integration, and enables users to
               create new data sets from those found on the Web. The key idea
               underlying Octopus is to offer the user a set of best-effort
               operators that automate the most labor-intensive tasks. For
               example, the Search operator takes a search-style keyword query
               and returns a set of relevance-ranked and similarity-clustered
               structured data sources on the Web; the Context operator helps
               the user specify the semantics of the sources by inferring
               attribute values that may not appear in the source itself, and
               the Extend operator helps the user find related sources that can
               be joined to add new attributes to a table. Octopus executes
               some of these operators automatically, but always allows the
               user to provide feedback and correct errors. We describe the
               algorithms underlying each of these operators and experiments
               that demonstrate their efficacy.",
  journal   = "Proceedings VLDB Endowment",
  publisher = "Association for Computing Machinery",
  volume    =  2,
  number    =  1,
  pages     = "1090--1101",
  year      =  2009
}

@ARTICLE{Bakarov2018-ga,
  title    = "A Survey of Word Embeddings Evaluation Methods",
  author   = "Bakarov, Amir",
  abstract = "Word embeddings are real-valued word representations able to
              capture lexical semantics and trained on natural language
              corpora. Models proposing these representations have gained
              popularity in the recent years, but the issue of the most
              adequate evaluation method still remains open. This paper
              presents an extensive overview of the field of word embeddings
              evaluation, highlighting main problems and proposing a typology
              of approaches to evaluation, summarizing 16 intrinsic methods and
              12 extrinsic methods. I describe both widely-used and
              experimental methods, systematize information about evaluation
              datasets and discuss some key challenges.",
  month    =  jan,
  year     =  2018
}

@INPROCEEDINGS{Camacho-Collados2016-ls,
  title     = "Find the word that does not belong: A Framework for an Intrinsic
               Evaluation of Word Vector Representations",
  author    = "Camacho-Collados, Jos{\'e} and Navigli, Roberto",
  abstract  = "We present a new framework for an intrin- sic evaluation of word
               vector representa- tions based on the outlier detection task.
               This task is intended to test the capabil- ity of vector space
               models to create se- mantic clusters in the space. We carried
               out a pilot study building a gold standard dataset and the
               results revealed two im- portant features: human performance on
               the task is extremely high compared to the standard word
               similarity task, and state- of-the-art word embedding models,
               whose current shortcomings were highlighted as part of the
               evaluation, still have consider- able room for improvement.",
  publisher = "Association for Computational Linguistics (ACL)",
  pages     = "43--50",
  month     =  aug,
  year      =  2016
}

@ARTICLE{Kamra2008-kl,
  title    = "Detecting anomalous access patterns in relational databases",
  author   = "Kamra, Ashish and Terzi, Evimaria and Bertino, Elisa",
  abstract = "A considerable effort has been recently devoted to the
              development of Database Management Systems (DBMS) which guarantee
              high assurance and security. An important component of any strong
              security solution is represented by Intrusion Detection (ID)
              techniques, able to detect anomalous behavior of applications and
              users. To date, however, there have been few ID mechanisms
              proposed which are specifically tailored to function within the
              DBMS. In this paper, we propose such a mechanism. Our approach is
              based on mining SQL queries stored in database audit log files.
              The result of the mining process is used to form profiles that
              can model normal database access behavior and identify intruders.
              We consider two different scenarios while addressing the problem.
              In the first case, we assume that the database has a Role Based
              Access Control (RBAC) model in place. Under a RBAC system
              permissions are associated with roles, grouping several users,
              rather than with single users. Our ID system is able to determine
              role intruders, that is, individuals while holding a specific
              role, behave differently than expected. An important advantage of
              providing an ID technique specifically tailored to RBAC databases
              is that it can help in protecting against insider threats.
              Furthermore, the existence of roles makes our approach usable
              even for databases with large user population. In the second
              scenario, we assume that there are no roles associated with users
              of the database. In this case, we look directly at the behavior
              of the users. We employ clustering algorithms to form concise
              profiles representing normal user behavior. For detection, we
              either use these clustered profiles as the roles or employ
              outlier detection techniques to identify behavior that deviates
              from the profiles. Our preliminary experimental evaluation on
              both real and synthetic database traces shows that our methods
              work well in practical situations. \copyright{} 2007
              Springer-Verlag.",
  journal  = "VLDB J.",
  volume   =  17,
  number   =  5,
  pages    = "1063--1077",
  month    =  aug,
  year     =  2008,
  keywords = "Anomaly detection; DBMS; Intrusion detection; RBAC; User profiles"
}

@ARTICLE{Granello2004-cz,
  title     = "Online data collection: Strategies for research",
  author    = "Granello, Darcy Haag and Wheaton, Joe E",
  abstract  = "Online data collection, through e-mail and Web-based surveys, is
               becoming an increasingly popular research methodology. In this
               article, the authors outline the benefits and limitations of
               this type of data collection to help researchers determine
               whether their data could be collected online in a way that
               retains the integrity of the data. A detailed procedure,
               including strategies to manage limitations, is given for
               researchers wishing to conduct their own online surveys",
  journal   = "J. Couns. Dev.",
  publisher = "Wiley Blackwell",
  volume    =  82,
  number    =  4,
  pages     = "387--393",
  year      =  2004
}

@INPROCEEDINGS{Chen2013-ye,
  title     = "Automatic web spreadsheet data extraction",
  booktitle = "{ACM} International Conference Proceeding Series",
  author    = "Chen, Zhe and Cafarella, Michael",
  abstract  = "Spreadsheets contain a huge amount of high-value data but do not
               observe a standard data model and thus are difficult to
               integrate. A large number of data integration tools exist, but
               they generally can only work on relational data. Existing
               systems for extracting relational data from spreadsheets are too
               labor intensive to support ad-hoc integration tasks, in which
               the correct extraction target is only learned during the course
               of user interaction. This paper introduces a system that
               automatically extracts relational data from spreadsheets,
               thereby enabling relational spreadsheet integration. The
               resulting integrated relational data can be queried directly or
               can be translated into RDF triples. When compared to standard
               techniques for spreadsheet data extraction on a set of 100
               random Web spreadsheets, the system reduces the amount of human
               labor by 72\% to 92\%. In addition to the system design, we
               present the results of a general survey of more than 400,000
               spreadsheets we downloaded from the Web, giving a novel view of
               how users organize their data in spreadsheets.",
  year      =  2013
}

@TECHREPORT{Brass_undated-ph,
  title    = "Detecting Semantic Errors in {SQL} Queries",
  author   = "Brass, Stefan and Goldberg, Christian and Hinneburg, Alexander",
  abstract = "We investigate classes of SQL queries which are syntactically
              correct, but certainly not intended, no matter for which task the
              query was written. For instance, queries that are contradictory,
              i.e. always return the empty set, are quite often written in
              exams of database courses. Current database management systems,
              e.g. Oracle, execute such queries without any warning. In this
              paper, we explain serveral classes of such errors, and give
              algorithms for detecting them. Of course, questions like the
              satisfiability are in general undecidable, but our algorithm can
              treat a significant subset of SQL queries. We believe that future
              database management systems will perform such checks and that the
              generated warnings will help to develop code with fewer bugs in
              less time."
}

@TECHREPORT{Melo_undated-lz,
  title     = "Semantic Web 0 (0) 1 Automatic Detection of Relation Assertion
               Errors and Induction of Relation Constraints",
  author    = "Melo, Andre and Paulheim, Heiko",
  abstract  = "Although the link prediction problem, where missing relation
               assertions are predicted, has been widely researched, error
               detection did not receive as much attention. In this paper, we
               investigate the problem of error detection in relation
               assertions of knowledge graphs, and we propose an error
               detection method which relies on path and type features used by
               a classifier for every relation in the graph exploiting local
               feature selection. Furthermore, we propose an approach for
               automatically correcting detected errors originated from
               confusions between entities. Moreover, we present an approach
               that translates decision trees trained for relation assertion
               error detection into SHACL-SPARQL relation constraints. We
               perform an extensive evaluation on a variety of datasets
               comparing our error detection approach with state-of-the-art
               error detection and knowledge completion methods, backed by a
               manual evaluation on DBpedia and NELL. We evaluate our error
               correction approach results on DBpedia and NELL and show that
               the relation constraint induction approach benefits from the
               higher expressiveness of SHACL and can detect errors which could
               not be found by automatically learned OWL constraints.",
  publisher = "IOS Press",
  keywords  = "error detection; knowledge graph; ontology learning; shacl;
               sparql"
}

@TECHREPORT{Sporleder_undated-xf,
  title    = "Spotting the 'Odd-one-out': {Data-Driven} Error Detection and
              Correction in Textual Databases",
  author   = "Sporleder, Caroline and Van Erp, Marieke and Porcelijn, Tijn and
              Van Den Bosch, Antal",
  abstract = "We present two methods for semi-automatic detection and
              correction of errors in textual databases. The first method
              (horizontal correction) aims at correcting inconsistent values
              within a database record, while the second (vertical correction)
              focuses on values which were entered in the wrong column. Both
              methods are data-driven and language-independent. We utilise
              supervised machine learning, but the training data is obtained
              automatically from the database; no manual annotation is
              required. Our experiments show that a significant proportion of
              errors can be detected by the two methods. Furthermore , both
              methods were found to lead to a precision that is high enough to
              make semi-automatic error correction feasible."
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@TECHREPORT{Sjobergh_undated-hb,
  title    = "Faking Errors to Avoid Making Errors: Machine Learning for Error
              Detection in Writing",
  author   = "Sj¨obergh, Jonas and Sj¨obergh, Sj¨ and Knutsson, Ola",
  abstract = "This paper describes a method to detect errors in written text
              which requires no manual work. The method used is to simply
              annotate a lot of errors in written text and train an
              off-the-shelf machine learning implementation to recognize such
              errors. To avoid manual annotation synthetically created errors
              are used for training. The method is evaluated on erroneously
              split compounds and word order errors. Results are comparable to
              a state of the art grammar checker based on manually created
              rules. The evaluation is performed on real (not synthetic)
              errors."
}

@ARTICLE{Krishnan2017-tj,
  title    = "{BoostClean}: Automated Error Detection and Repair for Machine
              Learning",
  author   = "Krishnan, Sanjay and Franklin, Michael J and Goldberg, Ken and
              Wu, Eugene",
  abstract = "Predictive models based on machine learning can be highly
              sensitive to data error. Training data are often combined with a
              variety of different sources, each susceptible to different types
              of inconsistencies, and new data streams during prediction time,
              the model may encounter previously unseen inconsistencies. An
              important class of such inconsistencies is domain value
              violations that occur when an attribute value is outside of an
              allowed domain. We explore automatically detecting and repairing
              such violations by leveraging the often available clean test
              labels to determine whether a given detection and repair
              combination will improve model accuracy. We present BoostClean
              which automatically selects an ensemble of error detection and
              repair combinations using statistical boosting. BoostClean
              selects this ensemble from an extensible library that is
              pre-populated general detection functions, including a novel
              detector based on the Word2Vec deep learning model, which detects
              errors across a diverse set of domains. Our evaluation on a
              collection of 12 datasets from Kaggle, the UCI repository,
              real-world data analyses, and production datasets that show that
              Boost- Clean can increase absolute prediction accuracy by up to
              9\% over the best non-ensembled alternatives. Our optimizations
              including parallelism, materialization, and indexing techniques
              show a 22.2x end-to-end speedup on a 16-core machine.",
  month    =  nov,
  year     =  2017
}

@ARTICLE{Park2015-xb,
  title     = "Error correction of reference indexing system including
               multimedia journals",
  author    = "Park, Jae Hwa and Park, Ho Hyun and Kwon, Young Bin",
  abstract  = "The Impact Factor evaluated of ISI is exploited as a standard to
               evaluate the level of academic journals including multimedia
               journals. However, the service for the impact factors is mainly
               designed for English-based journals only. A reference index
               system and database, `Science Citation Index Processing System:
               SCIPS' has been developed for reference indexing and to evaluate
               the impact factors and immediacy indices. And the developed
               system is applied for Korean domestic journals. It can be easily
               ported to other domestic journals in other languages. The
               proposed system has two major steps of indexing and computation.
               For the indexing process, an error correction system for
               references of papers is implemented. The typos of reference
               listing are analysed and the major types are categorized. Then
               the correction lookup table and indices have been built to
               calculate more accurate impact factors. In the computation step,
               the actual impact factors and immediacy indices for the journals
               listed in the database are calculated. For the overall process,
               journal databases and indexing system for the journal contents
               have been built. In the experiments, the data obtained from the
               actually published papers from 2009 to 2011 of Journals are used
               and impact factor and immediacy index of 2011 are calculated.
               The experimental results show the validity of presented system.
               From the tables of 2011 of high cited rankings, the well known
               society or famous conferences are cited highly. Also, some
               international standard documents are highly cited in
               communication area. Writing errors of approximately 10 \% in
               references degrades the evaluation accuracy of citation index
               and immediacy index. The error cases such as author error,
               abbreviation only, and unknown types are classified and analyzes
               to enhance the accuracy of the metrics.",
  journal   = "Multimed. Tools Appl.",
  publisher = "Kluwer Academic Publishers",
  volume    =  74,
  number    =  7,
  pages     = "2359--2370",
  year      =  2015,
  keywords  = "Error correction; Faulty Referencing; Impact Factor; Reference
               indexing; Science Citation Index"
}
