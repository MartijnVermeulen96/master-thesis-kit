\chapter{Background}
\label{chap:background}
The goal of error detection is to identify incomplete, incorrect, inaccurate or irrelevant parts of the data.
\\
\section{(Semi-)automated error detection}
% How are errors typically classified, identified etc..
% What are common type of approaches to find errors automatically
In order to automatically identify incomplete, incorrect, inaccurate or irrelevant parts of the data, numerous methods have been proposed in research. 

% Outliers
\blockquote{Characterizing,
locating, and in some cases eliminating these outliers offers
interesting insight about the data under scrutiny and reinforces
the confidence that one may have in conclusions drawn from
otherwise noisy datasets. \cite{Pit--Claudel2016-dj}}

% Duplicates
Duplicates are different tuples, referring to the same real-world entity. This could signify an error in the data, but could also be the 

% Rule violations

% Pattern violations



\subsection{Data processing flow}
% What would be the focus
% Where is it included

\subsection{Metrics}
% What are important things in the automated error detection field
\subsubsection{Automation}

\subsubsection{Benchmarks}
\paragraph{CleanML} 

\subsection{Datasets \& Error generation}
\subsubsection{Error types}

\subsubsection{Error generation}
\paragraph{BART}


\section{Error detection tools}
% Include most commonly used stuff

\subsection{Error Detection Methods \& Frameworks}


%% Regex
\paragraph{Regular expression violation}
    % In research, it is done by hand
    % Automatic Regex discovery
In many error detection methods, some form of text pattern recognition is 
    
%% (C)FDs
\paragraph{(Conditional) Functional Dependency violation}
    % Keys/Formal definition of relations
    % Automatic (C)FD detection
        % Problem with erroneous data (finding approximate rules)

% Denial constraints

%% NADEEF


%% dBoost
\paragraph{dBoost \cite{Pit--Claudel2016-dj}} is "Outlier Detection in Heterogeneous Datasets using Automatic Tuple Expansion". This works demonstrates the expansion of data tuples using knowledge about the schema and field types. For example, a timestamp 1424866716 could be expanded into year 2015, Wednesday, etc.. Then using statistical analysis of these expansions, the data is modeled using machine-learning
algorithms (Histograms, Gaussian, and Mixtures). Whenever a part of the tuple expansion does not lie in data model for a certain user-defined threshold, the entry is flagged as an outlier.



%% FAHES
\paragraph{FAHES \cite{Qahtan2018-te}} is a disguised missing values detector. Whereas most missing value detector focus only on NULL or empty values, this tool takes a different approach. They categorize detectable disguided missing values into five different cases: 
\begin{enumerate}
    \item Out of range data values
    \item Outliers
    \item String with repeated characters or characters that are next to each other on the used keyboard
    \item Values with non-conforming data types
    \item Valid values that are randomly distributed within the range of the data and used frequently in the data set
\end{enumerate}

Cases 1-4 can be treated as outliers, whereas case 5 is an inlier in the data distribution. 
The research shows detection using syntactic outliers where the syntactic pattern are discovered. By comparing pattern frequencies and checking for repeated patterns (for example pushing the same key over and over again), possible disguised missing values can be found. The research also proposes a more simple density based numeric outlier detection method. Lastly, detecting errors as inliers in statistical models is proposed. Following two statistical missing value models, MCAR (missing-completely-at-random) and MAR (missing-at-random), detecting the values that replace the missing values and used frequently could be achieved by removing one of the frequent values in a given attribute and testing if the resulting missing cells follow either models.

%% ForbiddenItemSets
\paragraph{Forbidden Itemsets \cite{Rammelaere2019-ea}} applies constraint-like method to detect and repair invalid entries in a dataset. However, it assume real-world scenarios, where constraints or rules specifying when data is dirty are not available. The proposed so-called forbidden itemsets capture unlikely value co-occurrences, similar to denial constraints. Experiments show high precision on detected errors. However, one main requirement is that a large body of clean data is needed to detect forbidden value combinations: if most of the data is dirty, inconsistencies are no longer “unlikely”, and errors will not be considered forbidden. 

%% HoloClean
\paragraph{HoloClean \cite{Rekatsinas2017-iw}}

%% KATARA
\paragraph{KATARA \cite{Chu2015-fs}}
The challenges tackled in this research are that:
\begin{itemize}
    \item Matching (dirty) tables to knowledge bases is a hard problem
    \item Knowledge bases are usually incomplete in terms of coverage of values in the table
    \item Human involvement is needed to validate matchings and to verify data when the knowledge-bases do not have enough coverage
\end{itemize}
The following contributions are made in this research:
\begin{itemize}
    \item Table pattern definition and discovery
    \item Table pattern validation via crowdsourcing
    \item Data annotation
\end{itemize}

% Llunatic
% Knime
% OpenRefine
% Pentaho
% DC-Clean
% Trifacta
% Data Tamer/TAMR
% SCARE
% BoostClean
% Uni-Detect
% ActiveClean

\subsection{State of the art}
% Show what is most promising
Combining all these different base techniques, the following methods proposed in research use a combination of previous knowledge to achieve state of the art error detection performance. 

% Auto-Detect
\paragraph{Auto-Detect (\cite{Wang2019-jg})} is a data-driven error detection approach (very similar to another recent tool Uni-Detect \cite{Huang2018-er}, but Auto-Detect outperforms Uni-Detect). It uses a large corpus consisting of 100M web tables extracted from a commercial search engine. Because it is extracted from a commercial search engine, the data quality is expected to be high. It tries to learn about the four common classes of errors. It performs single-column error detection and creates a generalization tree to translate actual values to patterns to compare. This tool however needs a specific corpus related to the subject dataset. While this tool performs really well in its own tests, it is not tested on common benchmark datasets. Also, the implementation of this tool is not available publicly.

% Metadata-driven error detection (DetectEr)
\paragraph{Metadata-Driven Error Detection \cite{Visengeriyeva2018-qz}} makes use of combining different algorithms by either bagging or stacking, and adds content-based metadata to the features to enhance error detection classification. Augmenting system features with automatically generated metadata information will improve the error predicting outcome. This insight might help future work. The base error detection strategies in the ensemble learning methods need to be chosen and configured by hand, which is again a hard process. Also, this work has not been developed any further for a few months and other methods outperform this now.

% HoloDetect
\paragraph{HoloDetect \cite{Heidari2019-ox}} is a few-shot learning framework for error detection that uses data augmentation to create more synthetic training examples resulting in high performance. It also has an expressive model to learn representations and syntactic and semantic differences in errors. This work uses a lot of feature engineering fed into a neural network. Feature engineering is hard and might influence the performance, especially because the exact implementation is not public. Also, depending on the amount of training time and samples, this method varies in performance. One other note is that the released $F_1$ scores are not calculated correctly for some of their tests, making HoloDetect come out on top every time, whereas it would not have been the best. The key takeaway is that data augmentation could solve the imbalanced data problem and even outperforms active learning methods, reducing human efforts. 

% UGuide
\paragraph{UGuide \cite{Thirumuruganathan2017-ip}} is an interactive tool that detects the set of functional dependency detectable errors under a fixed human effort budget. This work has a good focus on the human budget in data cleaning. One downside of this research is that all the errors were synthetically generated using BART \cite{Arocena2015-om}. It would be interesting to see if finding FDs will still be possible with highly erroneous datasets. It asks the user to verify tuple-based, fd-based and cell-based questions about the validity of dependencies. However, only using functional dependencies is not expressive enough to tackle the whole error detection problem, but the methodology of this work gave extra insights especially on human budget.


% Raha
\paragraph{Raha \cite{Mahdavi2019-zf}} is a configuration-free error detection system. Using the data profiles like in REDS \cite{Mahdavi2019-pk}, it selects different preconfigured strategies automatically, based on previously cleaned datasets. Then, it incorporates the outputs from various error detection strategies as a feature vector for the error detection task. 
Using these feature vectors, it creates clusters of which samples will be labeled in order to reduce user involvement. Drawbacks could be that information might get lost in the stacking process. Also, the (time) complexity of this tool increases vastly when adding more base learners.

% ED2
\paragraph{ED2 \cite{Neutatz2019-aw}} or Example-Driven Error Detection, is a machine learning based error detection technique that also makes use of active human labeling. 
Like HoloDetect \cite{Heidari2019-ox}, it creates features that cover information on the attribute, tuple (across attributes), and dataset level for each data cell of the dataset. It iteratively selects columns that will be sampled from, humanly labeled and propagated using different strategies. The main contribution is the effective selection of columns, reducing the user input effort by selecting columns \& cells to be labeled, that will have the most positive impact on the learning. As with all active learning tools, the drawback is the human interaction. The amount of examples needed to be labeled by the user differ vastly. Also, no confidence intervals are given for the performance metrics, making it unsure what the worst case performance will be. 


\section{Dataset profiling}


%% Why schema matching could be useful?
% Similar datasets are matched. Maybe, functions on similar datasets will give similar results.


%% A survey of approaches to automatic schema matching
\paragraph{A survey of approaches to automatic schema matching \cite{Rahm2001-ei}}. This research covers different schema matching approaches using a taxonomy. 
\blockquote{A fundamental operation in the manipulation of schema information is Match, which takes two schemas as input and produces a mapping between elements of the two schemas that correspond semantically to each other}
Historically, schema matching has been a part of 4 main processes. Schema integration (creating a global view of independent schemas), designing of source-to-warehouse transformations for raw input to a central warehouse with fixed schemas, e-commerce with a focus on message transformation and semantic query processing. 
The common divider in these processes, is the transformation of the input to some fixed known output.  
Then the research distinguished different approaches between schema and instance-level, element- and structure-level, and language and constraint-based matchers and discussed the combination of multiple matchers. Combining that with additional information (like dictionaries), will give a toolkit for matching schemas and showing similarities. 
Using all these tools with the purpose of error detection, could give insight on the performance of a tool on an unseen dataset, which has similar similarities to already known datasets.

%% Profiling relational data: a survey
\paragraph{Profiling relational data: a survey \cite{Abedjan2015-ul}} 
Goes with a focus on determining metadata about a given dataset. One of the insights is on the challenges of data profiling: (i) managing the input, (ii) performing the computation, and (iii) managing the output. For different input shapes and types, some certain transformation will be done beforehand. Also computing the different steps of data profiling might differ based on different content types, schemas or even completely different documents. Then managing the output to be useful for a specific task is a challenge on itself. Depending on the task, limitations on the output can be existing. 

\todo{Nog doen, zitten}

% Automatic dependency discovery
% Multicolumn - single column


%% REDS
% This is a direct descendant from the above schema matching ideas
\paragraph{REDS \cite{Mahdavi2019-pk}} estimates the performance of error detection strategies based on datasets using their so-called "dirtiness profiles". It takes a number of samples from the dataset, extracts content, structure and quality features from this sample. Then it trains a regression model to predict the performance for each strategy. For \textbf{N} strategies, there will be \textbf{N} different models to predict the performance.
This is promising as the performance does not require unrealistically large repositories. However, their estimation error was still increasing in their tests by adding more data profiles to the repository.
So a large repository could be possible and would lead to better estimations of performance of strategies.
It is used in Raha \cite{Mahdavi2019-zf}, but the idea could be used to improve explainability and predictability of performance of certain error detection tools.


% Machine Learning techniques for regression/clustering


\section{Feature selection \& importance}
