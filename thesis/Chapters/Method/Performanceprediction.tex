% RQ2
\section{Performance prediction}
\label{sec:performanceprediction}
The next step was to use the results given by the empirical study (\ref{sec:empiricalstudy}) for predicting performance on unseen new datasets. This idea was proposed by \cite{Mahdavi2019-pk}. This paper proposes the idea of a 'dirtiness profile'. The idea is that error detection tools would have similar performance on similar datasets. Regression models would be trained and tested on characteristics of the dataset, also called 'dirtiness profile' (input) and F1-scores (output). The following section will be a modification of the research by \cite{Mahdavi2019-pk}. 
The goal will be to estimate performance and predict values as close to the real performance results as possible, which can be measured with the mean squared error.
The train and test in and outputs are retrieved from the empirical study (section \ref{sec:empiricalstudy}). One main difference is that this research will solely focus on automated prediction, where no rules or patterns from the ground truth are taken and no user defined configurations need to be created in order to perform the predictions. Besides, multiple new input features will be introduced to see if the performance can be increased. Lastly, instead of directly estimating the F1-score, as done by \cite{Mahdavi2019-pk}, estimators will be trained on both the recall and precision scores. The F1-score can the be calculated using both estimated scores.
A regression model is trained for every strategy (error tool and specific configuration, see figure \ref{fig:eachstrategy}). 

\begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth]{thesis/Figures/EachStrategy.png}
	\caption{Each strategy will result in a separate estimator}
	\label{fig:eachstrategy}
\end{figure}

\subsection{Data profiles}
\label{subsec:dataprofiles}
The data profiles are created using only top-level features, without having any previous knowledge about the datasets. In this research, the focus lies only on features that are extractable without knowing the ground truth, only the dirty dataset. The basic set of features used is a subset of the proposed features from \cite{Mahdavi2019-pk}. However, all features where some information about the ground truth was necessary (rules, patterns or other heuristics found in the clean dataset), were left out. The extracted features extracted from each column. Then, aggregates of these column-wise extracted features are aggregated using the mean, variance, min and max to get a representation of feature distributions for each column in a dataset, with homogeneous output in size for training the regression models later on. These aggregated features representing the dataset profiles are the input values for the estimators, or the $x$ values. 

\begin{table}[H]
\centering
	\begin{tabular}{lll}
		Feature                   & Type       & Description \\ \hline
		characters\_unique        & characters &             \\
		characters\_alphabet      & characters &             \\
		characters\_numeric       & characters &             \\
		characters\_punctuation   & characters &             \\
		characters\_miscellaneous & characters &             \\
		words\_unique             & words      &             \\
		words\_alphabet           & words      &             \\
		words\_numeric            & words      &             \\
		words\_punctuation        & words      &             \\
		words\_miscellaneous      & words      &             \\
		words\_length             & words      &             \\
		cells\_unique             & cells      &             \\
		cells\_alphabet           & cells      &             \\
		cells\_numeric            & cells      &             \\
		cells\_punctuation        & cells      &             \\
		cells\_miscellaneous      & cells      &             \\
		cells\_length             & cells      &             \\
		cells\_null               & cells      &             
	\end{tabular}
	\caption{All extracted features for the dataset profiles}
	\label{tab:profilefeatures}
\end{table}
\todo{Add descriptions}

\subsection{Experimental setup}
%%% Leave-one-out training
% Real performance dataframe
% Estimation performance dataframe
% Calculate errors
To discover whether the discussed data profiles are useful in estimating the performance of error detection tools, the setup from section \ref{sec:empiricalstudy} is updated. Figure \ref{fig:estimateperformance_a} shows the training and testing part of the prediction. A data profile is extracted from 
Figure \ref{fig:estimateperformance_b} shows the goal of the estimation, where a new dataset is introduced, and the experimental part will be skipped altogether. The predictions of the experiment results will then be shown without having to run the actual tools.


\begin{figure}%[h]
	\centering
	\begin{subfigure}[b]{\textwidth}
		\includegraphics[width=1\linewidth]{thesis/Figures/PerformanceEstimation-Estimation.png}
		\caption{}
		\label{fig:estimateperformance_a} 
	\end{subfigure}
	
	\begin{subfigure}[b]{\textwidth}
		\includegraphics[width=1\linewidth]{thesis/Figures/PerformanceEstimation-Skip_real_run.png}
		\caption{}
		\label{fig:estimateperformance_b}
	\end{subfigure}
	\caption{Experiment result estimation}
\end{figure}

The estimators will be created as scikit-learn (\cite{Pedregosa2011-su}) pipelines. Not only does this allow interchangeable regression models, but also configurable data preparation. The different configurable parts all have certain assumptions. Because the underlying ideas behind the data profiles are not known, a power set (combinination of all possible settings), minus the unfeasible configurations will be created and ran as estimator. 
To evaluate the estimators, each estimator pipeline will be tested using leave-one-out cross validation. Each estimated result of the performance metric will be captured and kept for later comparison. The error of the estimated result will then be calculated. For each dataset $d$ from $D$ datasets and error detection strategy $s$ from all strategies $S$. An error is the difference between estimation of the score and the score itself. The real score is denoted as $y$, which can be anything that is a result of the empirical study, so mainly precision, recall and f1-score. The estimated score is denoted as $\hat{y}$. The error for a single score, for a specific error detection strategy and dataset is the following:

\begin{equation}
	e(s, d) = \hat{y}(s, d) - y(s, d)
\end{equation}

Estimators will be trained for every strategy. The leave-one-out cross validation will be done for each strategy, as show in table \ref{tab:leave-one-out}. 

\begin{table}%[H]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\cline{1-4} \cline{6-6}
\textbf{$d_1$} & \textbf{$d_2$} & \textbf{$d_3$} & \textbf{$d_4$} &                 & \textbf{Output} \\ \cline{1-4} \cline{6-6} 
\textit{Test}          & Train         & Train         & Train         & $\rightarrow$ & $y(s_i, d_1)$      \\ \cline{1-4} \cline{6-6} 
Train         & \textit{Test}          & Train         & Train         & $\rightarrow$ & $y(s_i, d_2)$      \\ \cline{1-4} \cline{6-6} 
Test          & Train         & \textit{Test}          & Train         & $\rightarrow$ & $y(s_i, d_3)$     \\ \cline{1-4} \cline{6-6} 
Test          & Train         & Train         & \textit{Test}          & $\rightarrow$ & $y(s_i, d_4)$      \\ \cline{1-4} \cline{6-6} 
\end{tabular}
\caption{Leave one out training example for a strategy $s_i$ and datasets $d_{1-4}$}
\label{tab:leave-one-out}
\end{table}

This will leave a $m \times n$ matrix of scores, representing a $m$ estimators and the $n$ available datasets, as shown for the real scores in table \ref{tab:realscores} and the estimated scores in \ref{tab:estimatedscores}. Subtracting the two table from each other will give the errors of estimation, also a $m \times n$ table of errors of estimation.

\begin{table}%[h]
	\centering
	\addtolength{\leftskip} {-2cm}
	\addtolength{\rightskip}{-2cm}
	\captionsetup[subtable]{position = below}
	\captionsetup[table]{position=top}
	\caption{$m \times n$ (strategies $\times$ datasets) matrix of scores}
	\begin{subtable}{0.5\linewidth}
		\centering
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			               & \textbf{$d_1$} & \textbf{$d_2$} & \textbf{...} & \textbf{$d_n$} \\ \hline
			\textbf{$s_1$} & $y(s_1, d_1)$  & $y(s_1, d_2)$  & ...          & $y(s_1, d_n)$  \\ \hline
			\textbf{$s_2$} & $y(s_2, d_1)$  & $y(s_2, d_2)$  & ...          & $y(s_2, d_n)$  \\ \hline
			\textbf{...}   & ...            & ...            & ...          & ...            \\ \hline
			\textbf{$s_m$} & $y(s_m, d_1)$  & $y(s_m, d_2)$  & ...          & $y(s_m, d_n)$  \\ \hline
		\end{tabular}
		\caption{Real scores}
		\label{tab:realscores}
	\end{subtable}%
	\hspace*{4em}
	\begin{subtable}{0.5\linewidth}
		\centering
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			               & \textbf{$d_1$}      & \textbf{$d_2$}      & \textbf{...} & \textbf{$d_n$}      \\ \hline
			\textbf{$s_1$} & $\hat{y}(s_1, d_1)$ & $\hat{y}(s_1, d_2)$ & ...          & $\hat{y}(s_1, d_n)$ \\ \hline
			\textbf{$s_2$} & $\hat{y}(s_2, d_1)$ & $\hat{y}(s_2, d_2)$ & ...          & $\hat{y}(s_2, d_n)$ \\ \hline
			\textbf{...}   & ...                 & ...                 & ...          & ...                 \\ \hline
			\textbf{$s_m$} & $\hat{y}(s_m, d_1)$ & $\hat{y}(s_m, d_2)$ & ...          & $\hat{y}(s_m, d_n)$ \\ \hline
		\end{tabular}
		\caption{Estimated scores}
		\label{tab:estimatedscores}
	\end{subtable}
\end{table}
       


To test the ability of an estimator pipeline to estimate the real scores of error detection strategies, the mean square error for all the (strategy, dataset) tuples is taken. 

\begin{equation}
	MSE = \frac{1}{|D||S|} \sum_{d \in D} \sum_{s \in S} e(s, d)^2
\end{equation}

This mean square error will give a method of comparison between different estimator configurations. For the best estimator configuration, the estimation error distribution, variance and 95\% percentile will be shown to give an overall report on the estimation correctness.

\subsection{Estimator selection}
\label{subsec:estimatorselection}
% Regression model selection
Now that a score for comparing estimators has been covered, the range of estimator configurations will be discussed.
The estimator pipeline consists of 4 possible parts:
\begin{enumerate}
	\item Feature normalization
	\item Feature selection
	\item Principle component analysis
	\item Regression model
\end{enumerate}

Each part of the pipeline is configurable. The first three parts can be left out, leaving only a regression model with the normal dataset profile as input. Most regression models are linear, are heavily impacted by outliers and might perform worse with high dimensional input values. 

\subsubsection{Feature normalization}
The three chosen possibilities for feature normalization are:
\begin{itemize}
	\item None: No normalization or scaling is applied.
	\item StandardScaler (\verb|sklearn.preprocessing.StandardScaler|): Standardize features by removing the mean and scaling to unit variance. The scaler will be trained on the training sample so to that after the training, each feature can be scaled using the distributions of the training set.
	\item Normalizer (\verb|sklearn.preprocessing.Normalizer|): Normalize samples individually to unit norm. Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm equals one. It is executed with a default $l_2$-norm
\end{itemize}

\subsubsection{Feature selection}
For feature selection, different trainable configurations are possible to optimize the amount of input dimensions for the later parts of the pipeline. Research has shown that using too many input features for machine learning will decrease performance (\cite{Trunk1979-sq}). To circumvent the curse of dimensionality, feature selection methods can be put in the machine learning pipeline to learn which features to use as well. 

\begin{itemize}
	\item None: No dimensionality reduction is applied in the pipeline.
	\item Variance Threshold (\verb|sklearn.feature_selection.VarianceThreshold|): This removes all features that in have a variance below a certain threshold in the training dataset. The threshold has to be set to some value.
	\item Select From Model (\verb|sklearn.feature_selection.SelectFromModel|): It selects the features based upon the importance weights of the trained regressor. On default, all features that have a weight above the mean of all feature weights are selected.
	\item Select K Best (\verb|sklearn.feature_selection.SelectKBest|): This method selects the K best features according to a specified score. In this case, the \\\verb|sklearn.feature_selection.f_regression| method is used, where the correlation between the target values and each separate (univariate) feature is calculated.
\end{itemize}

Besides feature selection that will be learned from the input features, are trainable and present in the machine learning pipeline, feature selection can be done in retrospect, which will be discussed in section \ref{subsec:featureselection}.

\subsubsection{Principle component analysis}
Principle component analysis is a dimensionality reduction method invented over a century ago (\cite{Pearson1901-de}), but still has a strong use case in machine learning today. As said in the previous section, too many input features will lead to worse performance with the small amount of input samples to train. A linear PCA will reduce the whole feature space to N number of components holding the most variance for all the input points. To transform and capture non-linear relations, also Kernel PCA will be used. This will be useful, as most regressorion models will not be capable of distinguishing between non-linear relations.

\begin{itemize}
	\item None: No dimensionality reduction using PCA will be done.
	\item PCA (\verb|sklearn.decomposition.PCA|): Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. Highly sensitive to scaling of the features (recommended to scale beforehand).
	\item Kernel PCA (\verb|sklearn.decomposition.KernelPCA|): Allows for non-linear dimensionality reudction through using specified kernels. A RBF (radial basis function) kernel is commonly used for projecting non-linear relation onto a linear separable projection.
\end{itemize}

\subsubsection{Regression model}
As the final step in the pipeline, a regression model is present. It takes in the input features from the last step in the pipeline, depending on which normalization, feature selcetion and/or principle component analysis methods are chosen. The regression model will take the transformed data profile inputs from all the training datasets and tries to estimate the selected real scores. This score always will be between 0 and 1, inclusive. So at the end of each estimator, a limiter will be put into place to not have any impossible scores. The possible regression models are stated below. All models have certain assumptions that the workings build upon. 

\begin{itemize}
	\item Linear Regression (\verb|sklearn.linear_model.LinearRegression|): This regression model fits a linear model that minimizes the sum of errors between the observed estimation and real observations. This method can be sensitive to outliers, due to its reliance on the square of errors.
	\item K-nearest Neighbors Regression (\verb|sklearn.neighbors.KNeighborsRegressor|): The output is estimated by comparing it to the nearest neighbors in the training samples. The K nearest points are weighted equally and the training scores will be combined as estimation.
	\item Ridge Regression (\verb|sklearn.linear_model.Ridge|): This model solves a regression model where the loss function is the linear least squares function and regularization is given by the $l_2$-norm
	\item Bayesian Ridge Regression (\verb|sklearn.linear_model.BayesianRidge|): This model can be used for estimating the regularization parameters as well, the regularization parameter is not set in a hard sense but tuned to the data at hand. It then estimates the ridge regression model.
	\item Decision Tree Regression (\verb|sklearn.tree.DecisionTreeRegressor|): This is a regression model that outputs different discrete output levels based on an outcome of a decision tree, comparing individual features to certain learned values.
	\item Support Vector Regression (\verb|sklearn.svm.SVR|): A support vector machine based regression model. Like normal Support Vector Classification, the regression model depends only on a subset of the training data, making it more robust against outliers.
	\item Gradient Boosting Regression (\verb|sklearn.ensemble.GradientBoostingRegressor|): Produces a predictive model from an ensemble of weak predictive models. In each stage a regression tree is fit on the negative gradient of the least squares regression loss.
	\item AdaBoost Regression (\verb|sklearn.ensemble.AdaBoostRegressor|): A meta-estimator that trains multiple regression models (default \verb|DecisionTreeRegressor|) and consequently adjust weights of input samples depending on the error of the previous estimator.
	\item Multi-layer Perceptron Regression (\verb|sklearn.neural_network.MLPRegressor|): A neural network based regression model. Capable of capturing non-linear relations, however that would require multiple layers, which will on its turn require more training samples. 
\end{itemize}

\subsection{Combination of estimators}
% Separate precision & recall estimator
% Combined values
Now that all the components for the estimator have been described, one can try to create ensemble estimators. In the study of \cite{Mahdavi2019-pk}, the F1-score was directly estimated. Knowing that this metric is directly build from both the precision and recall, it intuitively makes sense to first create estimators for the precision and recall (separate 2 estimator configurations), and use the output of these two configuration in combination to estimate the F1-score. Therefore, besides only calculating the MSE for a F1-score estimator, it will be compared to the MSE of the ensemble F1-score estimator.