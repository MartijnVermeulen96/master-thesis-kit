\section{Dataset profiling}
In this section dataset profiling will be discussed. Features need to be engineered to differentiate datasets in certain intrinsic properties. First, it will be covered on how a dataset profile could be of use for the estimation of performance of error detection strategies. Afterwards, different approaches and challenges for profiling relational data will be discussed.

%% REDS
% This is a direct descendant from the above schema matching ideas
\paragraph{REDS (\cite{Mahdavi2019-pk})} estimates the performance of error detection strategies based on datasets using their so-called "dirtiness profiles". It takes a number of samples from the dataset, extracts content, structure and quality features from this sample. Then it trains a regression model to predict the performance for each strategy. For \textbf{N} strategies, there will be \textbf{N} different models to predict the performance.
This is promising as their research did not require unrealistically large repositories. However, their estimation error was still increasing in their tests by adding more data profiles to the repository.
So a large repository could be possible and would lead to better estimations of performance of strategies.
The idea is also used in Raha \cite{Mahdavi2019-zf}, as a plugin, but the idea will be used in this research as a standalone recommendation for using error detection tools. The main idea is that similar datasets will have similar "dirtiness profiles". And similar datasets would require similar error detection methods. Key ideas from schema matching can be used to check for similarity between different datasets.

%% A survey of approaches to automatic schema matching
\paragraph{A survey of approaches to automatic schema matching (\cite{Rahm2001-ei})}. This research covers different schema matching approaches using a taxonomy. 
\blockquote{A fundamental operation in the manipulation of schema information is Match, which takes two schemas as input and produces a mapping between elements of the two schemas that correspond semantically to each other}
Historically, schema matching has been a part of 4 main processes. Schema integration (creating a global view of independent schemas), designing of source-to-warehouse transformations for raw input to a central warehouse with fixed schemas, e-commerce with a focus on message transformation and semantic query processing. 
The common divider in these processes, is the transformation of the input to some fixed known output.  
Then the research distinguished different approaches between schema and instance-level, element- and structure-level, and language and constraint-based matchers and discussed the combination of multiple matchers. Combining that with additional information (like dictionaries), will give a toolkit for matching schemas and showing similarities. 
Using all these tools with the purpose of error detection, could give insight on the performance of a tool on an unseen dataset, which has similar similarities to already known datasets.

%% Profiling relational data: a survey
\paragraph{Profiling relational data: a survey (\cite{Abedjan2015-ul})} 
Goes with a focus on determining metadata about a given dataset. One of the insights is on the challenges of data profiling: (i) managing the input, (ii) performing the computation, and (iii) managing the output. For different input shapes and types, some certain transformation will be done beforehand. Also computing the different steps of data profiling might differ based on different content types, schemas or even completely different documents. Then managing the output to be useful for a specific task is a challenge on itself. Depending on the task, limitations on the output can be existing. In the case of this thesis, the output must be homogeneous, in order to be used for a machine learning task. 
This limitation of fixed output, limits the possible profiling tasks that can be done or used in further calculations. According this work, data profiling can be split up in 3 types: single column profiling, multiple columns profiling and dependencies profiling. 
Single-column profiling has different tasks that can be directly converted to quantitative input features for a machine learning task. Examples of tasks are finding the number of null values, number of distinct values or calculating the percentage of cells that are of a number data type. These tasks can be executed with a single pass over the data, making the time complexity linear. 
Multi-column (for example clustering of rows) and dependencies (for example number of functional dependencies) profiling can be also aggregated to fixed numerical outputs. However, the main drawback of these techniques are the higher computational costs. These costs make the profiling methods less favored than the single-column profiling tasks.
So the main take-away from this work will be that there are different usable single-column profiling tasks that can be aggregated to fixed quantitative outputs usable for machine learning.